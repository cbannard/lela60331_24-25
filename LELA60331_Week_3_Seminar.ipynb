{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Week 3 Seminar Notebook\n",
        "This week we are going to be thinking about and working with language models. Along the way we will learn a few additional aspects of Python programming."
      ],
      "metadata": {
        "id": "QvSE04uu_Pe0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Lists\n",
        "\n",
        "We heard in week 1 about the different data types in Python. Another important  programming concept is the data structure - the structures used to store and organise data in our programs. The first of these we have encountered is the list. I have used this term informally in previous sessions because the everyday English word list capture quite well what a Python list is: an ordered store of entities, where those entities can be e.g. numbers, letter, words, other lists. We represent it using square brackets, such that we would create of list of integers from 1 to 10 like this:\n",
        "\n"
      ],
      "metadata": {
        "id": "9x3yCQbgF-aE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nums=[1,2,3,4,5,6,7,8,9,10]"
      ],
      "metadata": {
        "id": "akK3qq-yHcNh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can represent the first five letters of the alphabet like this:"
      ],
      "metadata": {
        "id": "DH6YV6PdHzZe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "alphabet=[\"a\",\"b\",\"c\",\"d\",\"e\"]"
      ],
      "metadata": {
        "id": "Yqx00ezYH6I9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can represent a sentence like this:"
      ],
      "metadata": {
        "id": "wAa6oiXBIDoC"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xO2OOTAUHgX_"
      },
      "outputs": [],
      "source": [
        "sentence = [\"this\", \"is\", \"a\", \"sentence\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "And we can represent a series of sentences like this"
      ],
      "metadata": {
        "id": "9umW5JK1IKXp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sentences = [[\"this\", \"is\", \"a\", \"sentence\"],[\"this\",\"is\",\"another\",\"sentence\"]]"
      ],
      "metadata": {
        "id": "B8oq0HR6IKmp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can print the contents of a list as a single string. The character in the quotes before \".join\" sets the character to be printed between the elements of the list. Here we use a space."
      ],
      "metadata": {
        "id": "ku11ujdgJRJN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "str.join(\" \", sentence)"
      ],
      "metadata": {
        "id": "5EWSCRztJWv6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6kGuJorOHgX_"
      },
      "source": [
        "We can also select elements from within the list. The entries in a list are indexed numberically starting with zero. So the first element is sentence[0] and the last element of this four element list is sentence[3]. These can then be select for printing as follows:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "69l83SK-HgYA"
      },
      "outputs": [],
      "source": [
        "sentence[2]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GS2RC2T8HgYA"
      },
      "source": [
        "We can also select subsequences of entries, by specifying a range as follows. Notice that the second character in the range isn't included - so 0:2 means from 0 up to the number before 2."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Zo9ZgHryHgYA"
      },
      "outputs": [],
      "source": [
        "str.join(\" \", sentence[0:2])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ciEFnb99HgYA"
      },
      "source": [
        "This allows us to, for example, insert elements in the middle of sentences as follows:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hn66kOmDHgYA"
      },
      "outputs": [],
      "source": [
        "print(str.join(\" \", sentence[0:3]) + \" short \" + sentence[3])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "An important thing to note is that a string in Python is a list of characters. So that you can select character from within strings as follows:"
      ],
      "metadata": {
        "id": "JM8O3DHrHfeM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sentence2 = \"the dog lay on the rug\""
      ],
      "metadata": {
        "id": "y0Rn22l-Lira"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentence2[5]"
      ],
      "metadata": {
        "id": "pDgdKw__LuNc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "You can find the length of a list as follows"
      ],
      "metadata": {
        "id": "LjB0bTCV2h33"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "len(sentence2)"
      ],
      "metadata": {
        "id": "f1YpuKdj2gaB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "You can iterate over the elements in a list as follows:"
      ],
      "metadata": {
        "id": "RssxlPiU2C8V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for i in sentence2:\n",
        "    print(i)"
      ],
      "metadata": {
        "id": "iGpLMLbc2HE_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Or by index as follows"
      ],
      "metadata": {
        "id": "erjcXH4t2dxN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(len(sentence2)):\n",
        "    print(sentence2[i])"
      ],
      "metadata": {
        "id": "1Ahy6EtB2U2u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "You can recombine the elements into a single string as follows:"
      ],
      "metadata": {
        "id": "h9eZyPyqIJMc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "s=\"\"\n",
        "for i in sentence2:\n",
        "    s += i\n",
        "s"
      ],
      "metadata": {
        "id": "MXPcQGoUIM5T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dictionaries\n",
        "A second useful data structure is the dictionary. This stores data in key and value pairs. There is a flexibility in the data types that can be keys and can be values, for example the former could be a string or an int. The latter could be a list or even another dictionary."
      ],
      "metadata": {
        "id": "rRhjvS71Mwk8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "thisdict = {\n",
        "  \"brand\": \"Ford\",\n",
        "  \"model\": \"Mustang\",\n",
        "  \"year\": 1964\n",
        "}"
      ],
      "metadata": {
        "id": "GzZyc9SENAqp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(thisdict)"
      ],
      "metadata": {
        "id": "hkBZ6gCANMrL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "You can obtain the keys as a standalone list"
      ],
      "metadata": {
        "id": "TpnWMsHuKxL5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "thisdict.keys()"
      ],
      "metadata": {
        "id": "NtrlqssuK3Il"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "And the same for the values"
      ],
      "metadata": {
        "id": "BoyoyS9qK68-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "thisdict.values()"
      ],
      "metadata": {
        "id": "tGy3_9SBK92h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "You can iterate over the keys and values of dictionaries as follows:"
      ],
      "metadata": {
        "id": "4LY_zLsR10_7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for key, value in thisdict.items():\n",
        "    print(key + \" \" + str(value))"
      ],
      "metadata": {
        "id": "xcamhP911V--"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "One useful additional thing to consider is that there are different kinds of dictionaries in the Collections library. We will make use of one special kind of dictionary - the default dictionary which returns a default value when asked for a missing key."
      ],
      "metadata": {
        "id": "y3zo9OcfBIsO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### The Shannon game\n",
        "\n",
        "Shannon (1951) described an ingenious way of estimating the entropy of English by using human predictions.\n",
        "\n",
        "Shannon, C. E. (1951). Prediction and entropy of printed English. Bell system technical journal, 30(1), 50-64.\n",
        "\n",
        "1. Play the Shannon game\n",
        "\n",
        "Copy this URL into a browser window and press return\n",
        "\n",
        "https://github.com/lianghuang3/shannon_game/archive/refs/heads/main.zip\n",
        "\n",
        "Unpack repository to somewhere on local machine\n",
        "\n",
        "If you are on University Computer or have VS Code installed\n",
        "Open VSCode (e.g. via Anaconda Navigator)\n",
        "Select File -> Open Folder and then select the shannon_game folder\n",
        "\n",
        "Open shannon_game.py and press Run\n",
        "\n",
        "Follow instructions"
      ],
      "metadata": {
        "id": "InlBGl8n9nGy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Calculate Entropy of English from a sample text"
      ],
      "metadata": {
        "id": "rGwfrYchCMDb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "# download from from the internt\n",
        "!wget https://www.gutenberg.org/files/2554/2554-0.txt\n",
        "# read in the file\n",
        "f = open('2554-0.txt')\n",
        "c_and_p = f.read()\n",
        "# select the first chapter - possible because I determined range\n",
        "chapter_one = c_and_p[5464:23725]\n",
        "# convert text to lower case\n",
        "chapter_one=chapter_one.lower()\n",
        "# remove all characters except from a-z and space\n",
        "chapter_one=re.sub('[^a-z ]','', chapter_one)"
      ],
      "metadata": {
        "id": "H4vb1HXphOEf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chapter_one"
      ],
      "metadata": {
        "id": "0pjZJj5Ynpcn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We are going to be assigning probabilities to sentences using bigram models, so we need to extract unigram and bigram counts. We will store them in dictionaries."
      ],
      "metadata": {
        "id": "irvJ9z_xIsXj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# First we will build a dictionary of unigram counts\n",
        "# We want to use a special kind of Dictionary called a default dictionary, so we have to import this\n",
        "from collections import defaultdict\n",
        "# Determine the total number of tokens (characters in this case) in the text\n",
        "total_unigrams = len(chapter_one)\n",
        "# Create an empty default dictionary\n",
        "unigrams = defaultdict(int)\n",
        "# Iterate through values of i from zero to the length of the text\n",
        "for i in range(total_unigrams):\n",
        "    # For each element in the list (characters in the chapter), increase the count of that word in our dictionary by one\n",
        "    unigrams[chapter_one[i]] += 1"
      ],
      "metadata": {
        "id": "f4LOgpkKokBv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "unigrams"
      ],
      "metadata": {
        "id": "afbjVTZRL4VY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Next we will build a dictionary of bigram counts\n",
        "# We want to use a special kind of Dictionary called a default dictionary, so we have to import this\n",
        "from collections import defaultdict\n",
        "# Determine the total number of bigram tokens (character bigrams in this case) in the text. This is the number of words minus 1\n",
        "total_bigrams = len(chapter_one) - 1\n",
        "# Create an empty default dictionary\n",
        "bigrams = defaultdict(int)\n",
        "# Iterate through values of i from zero to the length of the text minus 1\n",
        "for i in range(total_bigrams):\n",
        "     # For each element in the list (characters in the chapter) extract a bigram consisting of that element and the next and increase the count of that bigram in our dictionary by one\n",
        "    bigrams[chapter_one[i:i+2]] += 1\n"
      ],
      "metadata": {
        "id": "AskxxL7Ol_uK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bigrams"
      ],
      "metadata": {
        "id": "M4eU0xzDMnH9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we want to use these counts to calculate the entropy of English.\n",
        "\n",
        "If we were to calculate the entropy assuming every character was independent and occurred an equal number of times, the calculation would be very simple:"
      ],
      "metadata": {
        "id": "I_fuwg3WMtrb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "-math.log(1/27,2)"
      ],
      "metadata": {
        "id": "FFi77r3AswJY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We know that letters do not occur the same number of times (are not equiprobable) so the next thing we might try is to calculate the entropy reflecting the unequal rates but still assuming independence."
      ],
      "metadata": {
        "id": "dlbIANQjNC7Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# import the math library so that we can use the math.log function\n",
        "import math\n",
        "# initialise entropy to be zero\n",
        "H=0.0\n",
        "# Iterate over all key value pairs in our unigram count dictionary\n",
        "for key, value in unigrams.items():\n",
        "  # Calculate the unigram surprisal of each letter and add it to the entropy weighting by it relative frequency\n",
        "  H += -(value/total_unigrams * math.log(value/total_unigrams, 2))\n",
        "H"
      ],
      "metadata": {
        "id": "EI8_4dp1slMd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We know that characters are not independent - for each h is more likely to occur following t than following x. This is why we use bigram probabilities."
      ],
      "metadata": {
        "id": "PwsasxmoNWgi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# import the math library so that we can use the math.log function\n",
        "import math\n",
        "# initialise entropy to be zero\n",
        "H=0.0\n",
        "# Iterate over all key value pairs in our unigram count dictionary\n",
        "for key, value in bigrams.items():\n",
        "  # Identity the first element in the bigram\n",
        "  unikey = key[:1]\n",
        "  # Calculate the bigram surprisal for each bigram and add it to the entropy weighting by it relative frequency\n",
        "  H += -(value/total_bigrams * math.log(value/unigrams[unikey], 2))\n",
        "H"
      ],
      "metadata": {
        "id": "kWM5gYb8oWr5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The estimate is still higher than that we saw in the Shannon game. Why?"
      ],
      "metadata": {
        "id": "MT1xfkYGNjUZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Problem 1: Estimate entropy from a trigram character model. First of all you will need to rewrite the code to extract trigram counts."
      ],
      "metadata": {
        "id": "_iL4-2j9NrId"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# We want to use a special kind of Dictionary called a default dictionary, so we have to import this\n",
        "from collections import defaultdict\n",
        "# Determine the total number of trigram tokens (character trigrams in this case) in the text.\n",
        "total_trigrams = ???\n",
        "# Create an empty default dictionary\n",
        "trigrams = defaultdict(int)\n",
        "# Iterate through values of i from zero to ??\n",
        "for i in range(total_trigrams):\n",
        "     # For each element in the list (characters in the chapter) extract a trigram consisting of that element and the next 2 and increase the count of that bigram in our dictionary by one\n",
        "    trigrams[chapter_one[????]] += 1"
      ],
      "metadata": {
        "id": "C65BEbi5N4fU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next you will need to use these to calculate the trigram probabilities and combine to calculate the entropy"
      ],
      "metadata": {
        "id": "RcmRZAvxOgnj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "H=0.0\n",
        "for key, value in trigrams.items():\n",
        "  bigramkey = ????\n",
        "  H += -(value/??? * math.log(value/???, 2))\n",
        "H"
      ],
      "metadata": {
        "id": "M9uTPRxoOsjA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "As well as using the counts to estimate the global Entropy of English we can also calculate the log probability of individual sentences including those that weren't in the text from which we estimated our model:"
      ],
      "metadata": {
        "id": "gyizHKmuUuoe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define our sentence (a list of characters)\n",
        "sentence3=\" the cat sat on the mat\"\n",
        "# Initialise our log probability\n",
        "log_prob=0.0\n",
        "# Iterate through the sentence from 0 to the length of the sentence minus 1 (because working with bigrams)\n",
        "for i in range(len(sentence3)-1):\n",
        "  #Extract the current bigram of the input sentence\n",
        "  key = sentence3[i:i+2]\n",
        "  # Identity the first element in the bigram\n",
        "  unigram = sentence[3]\n",
        "  # Calculate the log bigram probability for the bigram and add it to the log bigram probability for the sentence\n",
        "  log_prob += -math.log(bigrams[key]/unigrams[unigram],2)\n",
        "log_prob\n"
      ],
      "metadata": {
        "id": "m3Bip0mItzyO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Because we are adding surprisal for each new element in the sentence, the result is length dependent. So we need to normalize for sentence length. This is down as follows as gives us a measure called Perplexity:"
      ],
      "metadata": {
        "id": "fzEnusR0d-X7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pow(log_prob, 1/(len(sentence3)-1))"
      ],
      "metadata": {
        "id": "hnzyH5ZdeUnz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Before looking any more at this we will switch from character-based to word-based models"
      ],
      "metadata": {
        "id": "K1Ri0dBEVGVQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Language modelling for words\n",
        "\n",
        "We are now going to switch the building and exploiting word-based language models. In order to do this we are going to have generate a list of words instead of a list of characters. We are going to do this using re.split()\n",
        "\n",
        "### re.split()\n",
        "re.split() takes a regular expression as a first argument (if you don't have a precompiled pattern) and string as second (or first if you have a precompiled pattern) argument, and splits the string into tokens divided by all substrings matched by the regular expression."
      ],
      "metadata": {
        "id": "N5r2qMzYxoG2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# download from from the internt\n",
        "!wget https://www.gutenberg.org/files/2554/2554-0.txt\n",
        "# read in the file\n",
        "f = open('2554-0.txt')\n",
        "c_and_p = f.read()\n",
        "# select the first chapter - possible because I determined range\n",
        "chapter_one = c_and_p[5464:23725]\n",
        "# convert text to lower case\n",
        "chapter_one=chapter_one.lower()\n",
        "chapter_one=re.sub('\\n',' ', chapter_one)\n",
        "chapter_one=re.sub('[^a-z ]','', chapter_one)\n",
        "chapter_one=re.split(\" \", chapter_one)\n"
      ],
      "metadata": {
        "id": "ToMjvvtfxrIA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "With the text in this new tokenised format, the same algorithm can be applied to extract unigram and bigram counts."
      ],
      "metadata": {
        "id": "tkHx5PhgWbaD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import defaultdict\n",
        "total_unigrams = len(chapter_one) - 1\n",
        "unigrams = defaultdict(int)\n",
        "for i in range(total_unigrams):\n",
        "    unigrams[chapter_one[i]] += 1"
      ],
      "metadata": {
        "id": "zy5066AoyvRT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import defaultdict\n",
        "total_bigrams = len(chapter_one) - 2\n",
        "bigrams = defaultdict(int)\n",
        "for i in range(total_bigrams):\n",
        "    bigrams[str.join(\" \",chapter_one[i:i+2])] += 1"
      ],
      "metadata": {
        "id": "IQOwFmDNy0QO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "unigrams"
      ],
      "metadata": {
        "id": "gZiLuezcWzIR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bigrams"
      ],
      "metadata": {
        "id": "kzFoLndPWz8m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "And the counts can be used to assign probabilities to sentences in exactly the same way:"
      ],
      "metadata": {
        "id": "M-eQ9svvW1Oy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sentence3=[\"a\",\"man\",\"was\",\"in\",\"the\",\"house\"]\n",
        "log_prob=0.0\n",
        "for i in range(len(sentence3)-1):\n",
        "  print(i)\n",
        "  key = str.join(\" \",sentence3[i:i+2])\n",
        "  unigram = sentence3[i]\n",
        "  log_prob += -math.log(bigrams[key]/unigrams[unigram],2)\n",
        "log_prob"
      ],
      "metadata": {
        "id": "rcLB9nfCzqPU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now try calculating the log probability for a different sentence of your own creation"
      ],
      "metadata": {
        "id": "t0MJt9ia6Oh_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sentence3=[]\n",
        "log_prob=0.0\n",
        "for i in range(len(sentence3)-1):\n",
        "  print(i)\n",
        "  key = str.join(\" \",sentence3[i:i+2])\n",
        "  unigram = sentence3[i]\n",
        "  log_prob += -math.log(bigrams[key]/unigrams[unigram],2)\n",
        "log_prob"
      ],
      "metadata": {
        "id": "ia4vrQw86YM7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This probably won't work because of unseen bigrams.\n",
        "\n",
        "We need to solve this problem via smoothing.\n",
        "\n",
        "For example, add-one smoothing:"
      ],
      "metadata": {
        "id": "ycwY-IA36Y2a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sentence3=[\"a\",\"man\",\"was\",\"in\",\"the\",\"house\",\"when\",\"the\",\"aardvark\",\"arrived\"]\n",
        "log_prob=0.0\n",
        "for i in range(len(sentence3)-1):\n",
        "  print(i)\n",
        "  key = str.join(\" \",sentence3[i:i+2])\n",
        "  unigram = sentence3[i]\n",
        "  # We add a count of one to each bigram and then add the vocabulary size (number of unique words) to the denominator\n",
        "  log_prob += -math.log((bigrams[key]+1)/(unigrams[unigram]+len(unigrams.keys())),2)\n",
        "log_prob"
      ],
      "metadata": {
        "id": "RMhPLL_U5PF5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Or alternatively, back off smoothing with interpolation:"
      ],
      "metadata": {
        "id": "dCCDnDh68ZtS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sentence3=[\"a\",\"man\",\"was\",\"in\",\"the\",\"house\",\"when\",\"the\",\"aardvark\",\"arrived\"]\n",
        "log_prob=0.0\n",
        "# These lambdas can change but should sum to 1\n",
        "lambda1 = 0.5\n",
        "lambda2 = 1 - lambda2\n",
        "\n",
        "for i in range(len(sentence3)-1):\n",
        "  print(i)\n",
        "  key = str.join(\" \",sentence3[i:i+2])\n",
        "  unigram = sentence3[i]\n",
        "  # We combine the unigram and the bigram probabilities, weighting each equally.\n",
        "  log_prob += -math.log((bigrams[key]/unigrams[unigram])*lambda1 + (unigrams[unigram]/total_unigrams)*lambda2,2)\n",
        "log_prob"
      ],
      "metadata": {
        "id": "F9a7sTCL_8yu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Problem 2 (this is difficult so don't worry if you cannot solve it now. Give it a go and then come back to it later in the semester when you have more programming experience. It will be a good check on your progress):\n",
        "\n",
        "Estimate a trigram word-based language model. This will require smoothing and you can employ both kinds. You should make use of the code you wrote for character-based trigram language models and the examples of smoothing above."
      ],
      "metadata": {
        "id": "77IdH9nXXFbV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Other authors\n",
        "\n",
        "Download and build language models from the following different writers - Shakespeare and James Joyce. What do you notice?"
      ],
      "metadata": {
        "id": "0zF50O0YB4Ua"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# The complete works of Shakespeare\n",
        "\n",
        "# download from from the internt\n",
        "!wget https://www.gutenberg.org/cache/epub/100/pg100.txt\n",
        "# read in the file\n",
        "f = open('pg100.txt')\n",
        "shakespeare = f.read()\n",
        "\n",
        "shakespeare=shakespeare.lower()\n",
        "shakespeare=re.sub('\\n',' ', shakespeare)\n",
        "shakespeare=re.sub('[^a-z ]','', shakespeare)\n",
        "shakespeare=re.split(\" \", shakespeare)\n",
        "\n",
        "from collections import defaultdict\n",
        "total_unigrams_shakespeare = len(shakespeare) - 1\n",
        "unigrams_shakespeare = defaultdict(int)\n",
        "for i in range(total_unigrams_shakespeare):\n",
        "    unigrams_shakespeare[shakespeare[i]] += 1\n",
        "\n",
        "total_bigrams_shakespeare = len(shakespeare) - 2\n",
        "bigrams_shakespeare = defaultdict(int)\n",
        "for i in range(total_bigrams_shakespeare):\n",
        "    bigrams_shakespeare[str.join(\" \",shakespeare[i:i+2])] += 1"
      ],
      "metadata": {
        "id": "uPd8ybWrBzQa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Repeat for James Joyce's Ulysses\n",
        "\n",
        "# download from from the internt\n",
        "!wget https://www.gutenberg.org/cache/epub/4300/pg4300.txt\n",
        "# read in the file\n",
        "f = open('pg4300.txt')\n",
        "jj = f.read()\n",
        "\n",
        "jj=jj.lower()\n",
        "jj=re.sub('\\n',' ', jj)\n",
        "jj=re.sub('[^a-z ]','', jj)\n",
        "jj=re.split(\" \", jj)\n",
        "\n",
        "from collections import defaultdict\n",
        "total_unigrams_jj = len(jj)\n",
        "unigrams_jj = defaultdict(int)\n",
        "for i in range(total_unigrams_jj):\n",
        "    unigrams_jj[jj[i]] += 1\n",
        "\n",
        "total_bigrams_jj = len(jj) - 1\n",
        "bigrams_jj = defaultdict(int)\n",
        "for i in range(total_bigrams_jj):\n",
        "    bigrams_jj[str.join(\" \",jj[i:i+2])] += 1"
      ],
      "metadata": {
        "id": "CE3mqdiLcwch"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "shakespeare_sentence=re.split(\" \",\"friends, romans, countrymen, lend me your ears; i come to bury Caesar, not to praise him. the evil that men do lives after them; the good is oft interred with their bones\")\n",
        "jamesjoyce_sentence=re.split(\" \", \"stately, plump buck mulligan came from the stairhead, bearing a bowl of lather on which a mirror and a razor lay crossed\")\n",
        "\n",
        "log_prob_shakespeare_on_shakespeare=0.0\n",
        "log_prob_jj_on_shakespeare=0.0\n",
        "for i in range(len(shakespeare_sentence)-1):\n",
        "  key = str.join(\" \",shakespeare_sentence[i:i+2])\n",
        "  unigram = shakespeare_sentence[i]\n",
        "  log_prob_shakespeare_on_shakespeare += -math.log((bigrams_shakespeare[key]+1)/(unigrams_shakespeare[unigram]+len(unigrams_shakespeare.keys())),2)\n",
        "  log_prob_jj_on_shakespeare += -math.log((bigrams_jj[key]+1)/(unigrams_jj[unigram]+len(unigrams_jj.keys())),2)\n",
        "print(\"Shakespeare sentence:\")\n",
        "pow(log_prob_shakespeare_on_shakespeare, 1/(len(shakespeare_sentence)-1))\n",
        "\n",
        "print(\"Shakespeare model perplexity: \" + str(pow(log_prob_shakespeare_on_shakespeare, 1/(len(shakespeare_sentence)-1))))\n",
        "print(\"James Joyce model perplexity: \" + str(pow(log_prob_jj_on_shakespeare, 1/(len(shakespeare_sentence)-1))))\n",
        "\n",
        "log_prob_shakespeare_on_jj=0.0\n",
        "log_prob_jj_on_jj=0.0\n",
        "for i in range(len(jamesjoyce_sentence)-1):\n",
        "  key = str.join(\" \",jamesjoyce_sentence[i:i+2])\n",
        "  unigram = jamesjoyce_sentence[i]\n",
        "  log_prob_shakespeare_on_jj += -math.log((bigrams_shakespeare[key]+1)/(unigrams_shakespeare[unigram]+len(unigrams_shakespeare.keys())),2)\n",
        "  log_prob_jj_on_jj += -math.log((bigrams_jj[key]+1)/(unigrams_jj[unigram]+len(unigrams_jj.keys())),2)\n",
        "print(\"James Joyce sentence:\")\n",
        "print(\"Shakespeare model perplexity: \" + str(pow(log_prob_shakespeare_on_jj, 1/(len(jamesjoyce_sentence)-1))))\n",
        "print(\"James Joyce model perplexity: \" + str(pow(log_prob_jj_on_jj, 1/(len(jamesjoyce_sentence)-1))))\n",
        "\n"
      ],
      "metadata": {
        "id": "LZGsolq-YF0Y"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
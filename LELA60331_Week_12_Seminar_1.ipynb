{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Week 12 Seminar 1 Notebook: Training a Softmax Classifier\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "61td5eSnl5aI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Problem 1: Given the following weights and input for a 3-class classifier perform the forward pass to generate the probability for each class.\n"
      ],
      "metadata": {
        "id": "mWv8pOTKf1YA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "weights=[[0.77, 0.63, 0.5][0.02, 0.74, 0.22],[0.2, 0.76, 0.17]]\n",
        "x=[2.33,3.72,1.0]"
      ],
      "metadata": {
        "id": "n86usvs_fX9Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Problem 2: With a true label of class 0 (from 0, 1 and 2) and the initial weights and inputs above perform the backwards pass and update the weights. The result should match that seen in this week's lecture."
      ],
      "metadata": {
        "id": "SL4cSPdjiMBN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we generate a full dataset for softmax classification"
      ],
      "metadata": {
        "id": "gzkxuED4Au40"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {
        "id": "rfJwFw6wBXTp"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "## Create simulated data\n",
        "np.random.seed(10)\n",
        "w1_center = (1, 3)\n",
        "w2_center = (3, 1)\n",
        "w3_center = (1, 1)\n",
        "w4_center = (3, 3)\n",
        "\n",
        "x=np.concatenate((np.random.normal(loc=w1_center,size=(20,2)),np.random.normal(loc=w2_center,size=(20,2)),np.random.normal(loc=w3_center,size=(10,2)),np.random.normal(loc=w4_center,size=(10,2))))\n",
        "\n",
        "labs=np.repeat([0,1,2],[20,20,20],axis=0)\n",
        "y=np.repeat(np.diag((1,1,1)),[20,20,20],axis=0)\n",
        "x=x.T\n",
        "x=np.array([x[0],x[1],[1] * len(x[0])])\n",
        "y=y.T"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Problem 3. Given the data generated above write the code to train a multiclass classifier to convergence."
      ],
      "metadata": {
        "id": "PwC7MNBqA_98"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Problem 4.  Write code to calculate the multiclass precision and recall using macroaveraging (see week 7 lectures slides pt 2 for a definition). The code for calculating precision and recall for binary classifiers from week 8 will provide a useful starting point."
      ],
      "metadata": {
        "id": "m3nRLvrCXucO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Building an intent classifier\n",
        "\n",
        "Below you can find data for the task of intent classification. This is an important step in modern task-based dialogue systems - given a particular piece of input from the speaker, the system tries to determine what goal the speaker is trying to achieve, in order that it can then produce an appropriate response.\n",
        "\n",
        "The data set pairs 2100 reviews with one of seven different intents:\n",
        "\n",
        "'PlayMusic', e.g. \"play easy listening\" \\\\\n",
        "'AddToPlaylist' e.g. \"please add this song to road trip\" \\\\\n",
        "'RateBook' e.g. \"give this novel 5 stars\" \\\\\n",
        "'SearchScreeningEvent' e.g. \"give me a list of local movie times\" \\\\\n",
        "'BookRestaurant' e.g. \"i'd like a table for four at 7pm at Asti\" \\\\\n",
        "'GetWeather' e.g. \"what's it like outside\" \\\\\n",
        "'SearchCreativeWork' \"show me the new James Bond trailer\" \\\\\n",
        "\n",
        "To import the data please run the following cells.\n"
      ],
      "metadata": {
        "id": "nsJveI-yYg4R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! wget https://raw.githubusercontent.com/cbannard/lela60331_24-25/refs/heads/main/intent_classification.csv"
      ],
      "metadata": {
        "id": "yqznaPDmMgoS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d8831954-ddbc-40bf-a049-6c545f5ba998"
      },
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-12-09 17:16:05--  https://raw.githubusercontent.com/cbannard/lela60331_24-25/refs/heads/main/intent_classification.csv\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.109.133, 185.199.110.133, 185.199.108.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.109.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 130852 (128K) [text/plain]\n",
            "Saving to: ‘intent_classification.csv’\n",
            "\n",
            "intent_classificati 100%[===================>] 127.79K  --.-KB/s    in 0.03s   \n",
            "\n",
            "2024-12-09 17:16:05 (3.68 MB/s) - ‘intent_classification.csv’ saved [130852/130852]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import the data into utterances and labels lists\n",
        "utterances=[]\n",
        "labels=[]\n",
        "\n",
        "with open(\"intent_classification.csv\") as f:\n",
        "   # iterate over the lines in the file\n",
        "   for line in f.readlines()[1:]:\n",
        "        # split the current line into a list of two element - the review and the label\n",
        "        fields = line.rstrip().split(',')\n",
        "        # put the current review in the reviews list\n",
        "        utterances.append(fields[0])\n",
        "        # put the current sentiment rating in the labels list\n",
        "        labels.append(fields[1])"
      ],
      "metadata": {
        "id": "6eSkDiJhM7Y9"
      },
      "execution_count": 78,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Select vocabulary for inclusion in one-hot representations\n",
        "from collections import Counter\n",
        "import re\n",
        "# Tokenise the text, turning a list of strings into a list of lists of tokens. We use very naive space-based tokenisation.\n",
        "tokenized_sents = [re.findall(\"[^ ]+\",txt) for txt in utterances]\n",
        "# Collapse all tokens into a single list\n",
        "tokens=[]\n",
        "for s in tokenized_sents:\n",
        "      tokens.extend(s)\n",
        "# Count the tokens in the tokens list. The returns a list of tuples of each token and count\n",
        "counts=Counter(tokens)\n",
        "# Sort the tuples. The reverse argument instructs to put most frequent first rather than last (which is the default)\n",
        "so=sorted(counts.items(), key=lambda item: item[1], reverse=True)\n",
        "# Extract the list of tokens, by transposing the list of lists so that there is a list of tokens a list of counts and then just selecting the former\n",
        "so=list(zip(*so))[0]\n",
        "# Select the firs 5000 words in the list\n",
        "type_list=so[0:2500]"
      ],
      "metadata": {
        "id": "ffVOxg1CM2gx"
      },
      "execution_count": 79,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a 2100 x 2500 matrix of zeros\n",
        "M = np.zeros((len(utterances), len(type_list)))\n",
        "#iterate over the reviews\n",
        "for i, utt in enumerate(utterances):\n",
        "    # Tokenise the current review:\n",
        "    tokens = re.findall(\"[^ ]+\",utt)\n",
        "    # iterate over the words in our type list (the set of 5000 words):\n",
        "    for j,t in enumerate(type_list):\n",
        "        # if the current word j occurs in the current review i then set the matrix element at i,j to be one. Otherwise leave as zero.\n",
        "        if t in tokens:\n",
        "              M[i,j] = 1"
      ],
      "metadata": {
        "id": "y8dAtmn_NOpP"
      },
      "execution_count": 80,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_ints=np.random.choice(len(utterances),int(len(utterances)*0.8),replace=False)\n",
        "remaining_ints=list(set(range(0,len(utterances))) - set(train_ints))\n",
        "test_ints=np.random.choice(len(remaining_ints),int(len(remaining_ints)*0.5),replace=False)\n",
        "dev_ints=list(set(range(0,len(remaining_ints))) - set(test_ints))"
      ],
      "metadata": {
        "id": "RH1JA9zGN0Ov"
      },
      "execution_count": 81,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "### Generate one-hot encoded input matrices for training, development and test\n",
        "M_train = np.array(M[train_ints,]).T\n",
        "M_test = np.array(M[test_ints,]).T\n",
        "M_dev = np.array(M[dev_ints,]).T\n",
        "\n",
        "unique_labels=list(set(labels))\n",
        "unique_one_hot=np.diag(np.ones(len(unique_labels)))\n",
        "\n",
        "labels_train = [labels[i] for i in train_ints]\n",
        "labels_test = [labels[i] for i in test_ints]\n",
        "labels_dev = [labels[i] for i in dev_ints]\n",
        "\n",
        "### Generate one-hot encoded target output matrices for training, development and test\n",
        "y_train=np.array([list(unique_one_hot[k]) for k in [unique_labels.index(x) for x in labels_train]]).T\n",
        "y_test=np.array([list(unique_one_hot[k]) for k in [unique_labels.index(x) for x in labels_test]]).T\n",
        "y_dev=np.array([list(unique_one_hot[k]) for k in [unique_labels.index(x) for x in labels_dev]]).T\n",
        "\n"
      ],
      "metadata": {
        "id": "i9Za2xE8ODu9"
      },
      "execution_count": 128,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The data has been split into training set of 1680, a development set of 210 and a test set of 210."
      ],
      "metadata": {
        "id": "4zFDhD0iZ7Wa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Problem 5. Train a softmax classifier for this task on the training set. You can test performance on the development set as you go."
      ],
      "metadata": {
        "id": "OlfEjSUlaHs0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Problem 6. Evaluate your classifier, by calculating precision and recall on the test set."
      ],
      "metadata": {
        "id": "Qa6-F7a5achg"
      }
    }
  ]
}
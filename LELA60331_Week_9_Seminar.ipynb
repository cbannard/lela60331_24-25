{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LtL_QtKk4ksZ"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# LELA60331 Week 9 Seminar\n",
        "\n",
        "This week we are going to look at multiclass classification and multilayer networks"
      ],
      "metadata": {
        "id": "JrWfN1IG43i5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Multiclass classification problems\n",
        "\n",
        "While logistic regression is great for binary classification tasks, many classification problems have more than two possible outcomes.  We can simulate such a situation as follows. I have just generalised sentiment analysis to a three class problem - negative, neutral and positive.\n",
        "\n"
      ],
      "metadata": {
        "id": "58xkbBkUzkVW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "## Create simulated data\n",
        "np.random.seed(10)\n",
        "w1_center = (1, 3)\n",
        "w2_center = (3, 1)\n",
        "w3_center = (1, 1)\n",
        "w4_center = (3, 3)\n",
        "\n",
        "x=np.concatenate((np.random.normal(loc=w1_center,size=(20,2)),np.random.normal(loc=w2_center,size=(20,2)),np.random.normal(loc=w3_center,size=(10,2)),np.random.normal(loc=w4_center,size=(10,2))))\n",
        "labs=np.repeat([0,1,2],[20,20,20],axis=0)\n",
        "y=np.repeat(np.diag((1,1,1)),[20,20,20],axis=0)\n",
        "x=x.T\n",
        "y=y.T"
      ],
      "metadata": {
        "id": "k-yx9h_40DIl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.scatter(x[0][labs==0], x[1][labs==0], marker='*', s=100)\n",
        "plt.scatter(x[0][labs==1], x[1][labs==1], marker='o', s=100)\n",
        "plt.scatter(x[0][labs==2], x[1][labs==2], marker='x', s=100)\n",
        "plt.xlabel(\"log count of negative words\")\n",
        "plt.ylabel(\"log count of positive words\")\n",
        "plt.xlim((0,5))\n",
        "plt.ylim((0,5))\n"
      ],
      "metadata": {
        "id": "I3japJAV50NE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "### Softmax\n",
        "In such circumstances we need to use multinomial logistic (aka softmax) regression.\n",
        "\n",
        "In logistic regression we take the dot product between our feature vector for each data point and our weight vector. We then add the bias to give us a single z value which we feed through the sigmoid function. We can have only one z values because there are only two outcomes and the following relationship holds:\n",
        "p(y=0|x) = 1-p(y-1)\n",
        "\n",
        "In multinomial regression we instead have a z value for each of our possible outcomes. We can use these collectively to calculate probabilties for each of our possible outcomes. For example if we had three possible outcomes, 0, 1 or 2 then we would calculate their probabilities as follows:\n",
        "\n",
        "$p(y=0|x) = \\frac{exp(z_{0})}{\\sum_{i,N} exp(z_i)}$ \\\\\n",
        "$p(y=1|x) = \\frac{exp(z_{1})}{\\sum_{i,N} exp(z_i)}$ \\\\\n",
        "$p(y=2|x) = \\frac{exp(z_{2})}{\\sum_{i,N} exp(z_i)}$ \\\\\n"
      ],
      "metadata": {
        "id": "v6QacNfX8TPj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Problem 1: A fitted model might return the following weights. In Python calculate the probabilites of each of the output classes for the following inputs. \\\\\n",
        "\n",
        "\n",
        "a) x[0] (positive words) = 10, x[1] (negative words) = 3 \\\\\n",
        "a) x[0] (positive words) = 3, x[1] (negative words) = 3 \\\\\n",
        "a) x[0] (positive words) = 1, x[1] (negative words) = 6 \\\\\n"
      ],
      "metadata": {
        "id": "gF0M7lm6k3O-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "bias_negative=-0.82031125\n",
        "bias_positive=-0.451126\n",
        "bias_neutral = 1.27143725\n",
        "\n",
        "weights_negative = np.array([-0.69900716, 1.81182487])\n",
        "weights_positive = np.array([1.7979912 , -0.74611263])\n",
        "weights_neutral = np.array([0.80449184, -0.07135976])"
      ],
      "metadata": {
        "id": "ckqrbHUIHlON"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Note: for convenience you can print a float with scientific notation with the  function np.format_float_positional, as in the following:"
      ],
      "metadata": {
        "id": "6WD2NE3nOYX4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x=1/783618\n",
        "x"
      ],
      "metadata": {
        "id": "bWU6LISqOd0t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "np.format_float_positional(x)"
      ],
      "metadata": {
        "id": "-A8mqC3fOiEO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Representing multinomial logistic regression problems\n",
        "\n",
        "In multinomial logistic regression we have multiple outcome classes. In place of the single 0 or 1 that we used as outcome in binary logistic regression, we represent the outcome using a vector of 0s and 1, with each position in the vector corresponding to one of the output classes. \\\\\n",
        "\n",
        "positive = [1,0,0] \\\\\n",
        "negative = [0,1,0] \\\\\n",
        "neutral = [0,0,1] \\\\\n",
        "\n",
        "This is how the y variable looks in our simulated data:"
      ],
      "metadata": {
        "id": "BG8JpQ-yPbE7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "y"
      ],
      "metadata": {
        "id": "Xft3loAtPYEY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y.T[1:20]"
      ],
      "metadata": {
        "id": "hcSULKmmQeF9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Fitting multinomial logistic regression models\n",
        "\n",
        "The relationship between this representation and the way we represent binary logistic regression is helpful in generalising the process of model fitting:\n",
        "\n",
        "Instead of using a regression equation to predict a single z value, in an n-class classifying we use n regression equations to predict n z values. These are then converted to n probabilties using the softmax function.\n",
        "\n",
        "When calculating loss and gradients in binary logistic regression we look at\n",
        "the difference between a single probability estimate and a single binary value for each datapoint for each of the m weights/input features. So that the gradient that we use to update a weight i is the mean of the following value over all datapoints:\n",
        "\n",
        "$ g_i =  (q - y) * x_{i}  \\\\ =     (p(y = 1|x) - y) * x_{i}$\n",
        "\n",
        "In multinomial logistic regression, we compare each of our n probabilities to each of n binary values when updating each of our n x k weights for each of our k input features.  So for m classes 0, 1 and 2 we would calculate the average of the following over all datapoints:\n",
        "\n",
        "$ g_{i}^{0} = (q^{0} - y^{0}) * x_{i}    =     (p(y^{0} = 1|x) - y^{0}) * x_{i}$\n",
        "\n",
        "$ g_{i}^{1} = (q^{1} - y^{1}) * x_{i}    =     (p(y^{1} = 1|x) - y^{0}) * x_{i}$  \n",
        "\n",
        "$ g_{i}^{2} = (q^{2} - y^{2}) * x_{i}    =     (p(y^{2} = 1|x) - y^{0}) * x_{i}$  \n"
      ],
      "metadata": {
        "id": "SBaLVvbiQkb4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Problem 2: Complete the code below so that it fits a softmax regression to our multiclass data"
      ],
      "metadata": {
        "id": "ZW-uMSS9OSyV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "np.random.seed(10)\n",
        "n_iters = 2500\n",
        "num_features=2\n",
        "num_classes=3\n",
        "num_samples = len(y[0])\n",
        "weights = np.random.rand(num_classes,num_features)\n",
        "bias=np.zeros(num_classes)\n",
        "lr=0.1\n",
        "logistic_loss=[]\n",
        "z=np.zeros((num_samples,num_classes))\n",
        "q=np.zeros((num_samples,num_classes))\n",
        "\n",
        "for i in range(n_iters):\n",
        "    z[:,0]=???\n",
        "    z[:,1]=???\n",
        "    z[:,2]=???\n",
        "\n",
        "    q[:,0] = ???\n",
        "    q[:,1] = ???\n",
        "    q[:,2] = ???\n",
        "\n",
        "    loss = sum(-(y[0]*np.log2(q[:,0])+(1-y[0])*np.log2(1-q[:,0])))/num_samples\n",
        "    loss += sum(-(y[1]*np.log2(q[:,1])+(1-y[1])*np.log2(1-q[:,1])))/num_samples\n",
        "    loss += sum(-(y[2]*np.log2(q[:,2])+(1-y[2])*np.log2(1-q[:,2])))/num_samples\n",
        "    logistic_loss.append(loss)\n",
        "\n",
        "    dw01 = ???\n",
        "    dw02 = ???\n",
        "\n",
        "    dw11 = ???\n",
        "    dw12 = ???\n",
        "\n",
        "    dw21 = ???\n",
        "    dw22 = ???\n",
        "\n",
        "    db0 = ???\n",
        "    db1 = ???\n",
        "    db2 = ???\n",
        "\n",
        "    weights[0,0] = weights[0,0] - dw01*lr\n",
        "    weights[0,1] = weights[0,1] - dw02*lr\n",
        "\n",
        "    weights[1,0] = weights[1,0] - dw11*lr\n",
        "    weights[1,1] = weights[1,1] - dw12*lr\n",
        "\n",
        "    weights[2,0] = weights[2,0] - dw21*lr\n",
        "    weights[2,1] = weights[2,1] - dw22*lr\n",
        "\n",
        "    bias[0] = bias[0] - db0*lr\n",
        "    bias[1] = bias[1] - db1*lr\n",
        "    bias[2] = bias[2] - db2*lr\n",
        "\n",
        "plt.plot(range(1,n_iters),logistic_loss[1:])\n",
        "plt.xlabel(\"number of epochs\")\n",
        "plt.ylabel(\"loss\")"
      ],
      "metadata": {
        "id": "mtUjZzhE60CO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Problem 3 (difficult addition to try in your own time if you feel able). Rewrite your softmax regression so that it calculates z, q, gradients and weights in single lines using operations over whole matrices rather than subsetting for particular rows, columns or elements. See assigned readings on linear algebra to remind you of numpy operations if needed."
      ],
      "metadata": {
        "id": "aa6Hk7w1sv5B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "np.random.seed(10)\n",
        "n_iters = 2500\n",
        "num_features=2\n",
        "num_classes=3\n",
        "num_samples = len(y[0])\n",
        "weights = np.random.rand(num_classes,num_features)\n",
        "bias=np.zeros(num_classes)\n",
        "lr=0.1\n",
        "logistic_loss=[]\n",
        "z=np.zeros((num_samples,num_classes))\n",
        "q=np.zeros((num_samples,num_classes))\n",
        "\n",
        "for i in range(n_iters):\n",
        "    z=???\n",
        "\n",
        "    q = ???\n",
        "    q = ???\n",
        "    q = ???\n",
        "\n",
        "    loss = sum(-(y[0]*np.log2(q[:,0])+(1-y[0])*np.log2(1-q[:,0])))/num_samples\n",
        "    loss += sum(-(y[1]*np.log2(q[:,1])+(1-y[1])*np.log2(1-q[:,1])))/num_samples\n",
        "    loss += sum(-(y[2]*np.log2(q[:,2])+(1-y[2])*np.log2(1-q[:,2])))/num_samples\n",
        "    logistic_loss.append(loss)\n",
        "\n",
        "    dw = ??\n",
        "    db = ??\n",
        "\n",
        "\n",
        "    weights = ??\n",
        "    bias = ??\n",
        "\n",
        "plt.plot(range(1,n_iters),logistic_loss[1:])\n",
        "plt.xlabel(\"number of epochs\")\n",
        "plt.ylabel(\"loss\")"
      ],
      "metadata": {
        "id": "NOsmfXmkthPB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Multilayer neural networks"
      ],
      "metadata": {
        "id": "FbQ01QICpiPT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Problem 4: Given the weights and the input vector below, write the code to calculate the predicted output value y_hat"
      ],
      "metadata": {
        "id": "iaSB8jrvszef"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import numpy as np\n",
        "weights_0_1=np.array([[-.59,.75,-.95],[.34,-.17,.12],[-.72,-.6,.6]])\n",
        "weights_1_2=np.array([[.93],[-.37],[.38]])\n",
        "alpha=0.2\n",
        "layer_0 = np.array([[ 0, 1, 1 ]])\n",
        "\n"
      ],
      "metadata": {
        "id": "zzBX-Cf2pu8E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Problem 5: Given the true label y=1 as specified below, write the code to perform the backwards pass and update the weights."
      ],
      "metadata": {
        "id": "H8F0uocJtUMk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "true_label = np.array([[1]])"
      ],
      "metadata": {
        "id": "VwnQGetMtGLR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Problem 6. Given the training data below, complete the code to train the network to solve the problem. You can use the code in the block below the training block to test that the models predictions from the training inputs are approximately correct."
      ],
      "metadata": {
        "id": "NJgzQAUutghP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "inputs = np.array( [[ 0, 0, 1 ],\n",
        "                          [ 0, 1, 1 ],\n",
        "                          [ 1, 0, 1 ],\n",
        "                          [ 1, 1, 1 ] ] )\n",
        "\n",
        "true_labels = np.array([ [0], [1], [1], [0]])"
      ],
      "metadata": {
        "id": "qihQlm_8tpzk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "alpha = 0.2\n",
        "num_features=3\n",
        "hidden_size = 3\n",
        "np.random.seed(1)\n",
        "weights_0_1 = np.random.rand(num_features,hidden_size)\n",
        "np.random.seed(1)\n",
        "weights_1_2 = np.random.rand(hidden_size,1)\n",
        "\n",
        "loss = []\n",
        "n_iters=1000\n",
        "for iteration in range(n_iters):\n",
        "   layer_2_error = 0\n",
        "   for i in range(len(inputs)):\n",
        "      layer_0 = inputs[i]\n",
        "      ## Add forward pass\n",
        "      ## Add backward pass and update weights\n",
        "\n",
        "   loss.append(layer_2_error)\n",
        "plt.plot(range(1,n_iters),loss[1:])\n",
        "plt.xlabel(\"number of epochs\")\n",
        "plt.ylabel(\"loss\")"
      ],
      "metadata": {
        "id": "09ebKssxuESB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for k in range(4):\n",
        "  layer_0 = inputs[k]\n",
        "  layer_1 = np.maximum(np.dot(layer_0,weights_0_1),0)\n",
        "  layer_2 = np.dot(layer_1,weights_1_2)\n",
        "  print(layer_2)"
      ],
      "metadata": {
        "id": "bG352ctkwWOc"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
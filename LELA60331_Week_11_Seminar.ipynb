{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8PIrfoQLCC5j"
      },
      "source": [
        "# Week 11 Seminar notebook: Embeddings\n",
        "\n",
        "Today we are going to look at how to make use of embeddings as a way of representing words in a simple classifier."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mOYxlomNCZMN"
      },
      "source": [
        "### Using Word2vec embeddings in a classifier\n",
        "\n",
        "We are going to build a sentiment classifier using the Yelp sentiment classification data we first saw in week 7. Instead of one-hot encoding we will use embeddings to represent our words.\n",
        "\n",
        "First we are going to import the pre-built embeddings and combine them to represent our texts. Note: The first block is included for reference and will not be used in the seminar- we will import precompiled representation of our reviews. DO NOT RUN THE FIRST BLOCK IN THE SEMINAR."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HrhIdSQ_7UGz"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Download w2v embeddings\n",
        "import gensim.downloader as api\n",
        "w = api.load('word2vec-google-news-300')\n",
        "vocab=[x for x in w.key_to_index.keys()]\n",
        "\n",
        "!wget https://raw.githubusercontent.com/cbannard/lela60331_24-25/refs/heads/main/data/yelp_reviews.txt\n",
        "\n",
        "# Create lists\n",
        "reviews=[]\n",
        "labels=[]\n",
        "\n",
        "with open(\"yelp_reviews.txt\") as f:\n",
        "   # iterate over the lines in the file\n",
        "   for line in f.readlines()[1:]:\n",
        "        # split the current line into a list of two element - the review and the label\n",
        "        fields = line.rstrip().split('\\t')\n",
        "        # put the current review in the reviews list\n",
        "        reviews.append(fields[0])\n",
        "        # put the current sentiment rating in the labels list\n",
        "        labels.append(fields[1])\n",
        "\n",
        "tokenized_sents = [re.findall(\"[^ ]+\",txt) for txt in reviews]\n",
        "# Collapse all tokens into a single list\n",
        "tokens=[]\n",
        "for s in tokenized_sents:\n",
        "      tokens.extend(s)\n",
        "# Count the tokens in the tokens list. The returns a list of tuples of each token and count\n",
        "types=set(tokens)\n",
        "\n",
        "indices=[vocab.index(x) for x in types if x in vocab]\n",
        "types_inc=[x for x in types if x in vocab]\n",
        "M=w[indices]\n",
        "M.shape\n",
        "\n",
        "import re\n",
        "embeddings=[]\n",
        "#iterate over the reviews\n",
        "for i, rev in enumerate(reviews):\n",
        "    # Tokenise the current review:\n",
        "    tokens = re.findall(\"[^ ]+\",rev)\n",
        "    this_vec = np.zeros((1, 300))\n",
        "    for t in tokens:\n",
        "        if t in types_inc:\n",
        "            #print(t)\n",
        "            #print(M[types_inc.index(t)])\n",
        "            this_vec = this_vec + M[types_inc.index(t)]\n",
        "    embeddings.append(this_vec)\n",
        "embeddings=np.array(embeddings).squeeze()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "To save time in the seminar we are going to import prebuilt embedding representations of our texts"
      ],
      "metadata": {
        "id": "P8oJ9E-UQx2j"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "teAPu8LLPQ5n"
      },
      "outputs": [],
      "source": [
        "!wget https://raw.githubusercontent.com/cbannard/lela60331_24-25/refs/heads/main/review_embeddings.csv.gz\n",
        "!gunzip review_embeddings.csv.gz\n",
        "!wget https://raw.githubusercontent.com/cbannard/lela60331_24-25/refs/heads/main/data/yelp_reviews.txt"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can build training and test data from the imported data as follows"
      ],
      "metadata": {
        "id": "bGtzKgGZU3G-"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sLi6ErwYSC6c"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "labels=[]\n",
        "\n",
        "with open(\"yelp_reviews.txt\") as f:\n",
        "   # iterate over the lines in the file\n",
        "   for line in f.readlines()[1:]:\n",
        "        # split the current line into a list of two element - the review and the label\n",
        "        fields = line.rstrip().split('\\t')\n",
        "        labels.append(fields[1])\n",
        "\n",
        "embeddings=[]\n",
        "with open(\"review_embeddings.csv\") as f:\n",
        "   # iterate over the lines in the file\n",
        "   for line in f.readlines():\n",
        "        # split the current line into a list of two element - the review and the label\n",
        "        fields = line.rstrip().split(',')\n",
        "        emb = [float(x) for x in fields]\n",
        "        embeddings.append(emb)\n",
        "\n",
        "\n",
        "embeddings=np.array(embeddings).squeeze()\n",
        "train_ints=np.random.choice(len(embeddings),int(len(embeddings)*0.8),replace=False)\n",
        "test_ints=list(set(range(0,len(embeddings))) - set(train_ints))\n",
        "# These are the training embeddings\n",
        "M_train_emb = embeddings[train_ints,]\n",
        "# These are the test embeddings\n",
        "M_test_emb = embeddings[test_ints,]\n",
        "# These are the training labels\n",
        "labels_train = [labels[i] for i in train_ints]\n",
        "# These are the test labels\n",
        "labels_test = [labels[i] for i in test_ints]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R_h-wBpcfCZT"
      },
      "source": [
        "\n",
        "\n",
        "Problem 1. Fit logistic regression models using the embedding-based representation of the reviews data. Train on the training data. Calculate precision and recall for the classifier on the test data. You should have example code for all the steps in previous weeks that will just need tweaking. Note that the embeddings are 300 elements long, so a weight vector of 300 is needed."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "num_features=300\n",
        "weights = np.random.rand(num_features)\n",
        "bias=np.random.rand(1)"
      ],
      "metadata": {
        "id": "lXITpMbKE6gn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "y=[int(l == \"positive\") for l in labels_train]"
      ],
      "metadata": {
        "id": "oKCq-1CfDrsD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Problem 2. Fit a logistic regression model using the one hot representations of reviews using the following prebuilt one-hot encodings. Test this model in the same way and compare performance. Note that the one-hot vectors are 5000 elements long (the chosen vocab was 5000; see week 7)."
      ],
      "metadata": {
        "id": "D_XyKo3_UtB7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://raw.githubusercontent.com/cbannard/lela60331_24-25/refs/heads/main/reviews_onehot.csv.gz\n",
        "!gunzip reviews_onehot.csv.gz"
      ],
      "metadata": {
        "id": "KKaoJm1zRCbU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "onehot=[]\n",
        "with open(\"reviews_onehot.csv\") as f:\n",
        "   # iterate over the lines in the file\n",
        "   for line in f.readlines():\n",
        "        # split the current line into a list of two element - the review and the label\n",
        "        fields = line.rstrip().split(',')\n",
        "        oh = [float(x) for x in fields]\n",
        "        onehot.append(oh)\n",
        "onehot=np.array(onehot).squeeze()\n",
        "# These are the training set one-hot vectors\n",
        "M_train_oh = onehot[train_ints,]\n",
        "# These are the test set one-hot vectors\n",
        "M_test_oh = onehot[test_ints,]\n",
        "# The labels are the same as for the embeddings as the reviews are the same"
      ],
      "metadata": {
        "id": "w3WzriTwRfTS"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
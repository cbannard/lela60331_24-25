{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# LELA60331 Week 5 Seminar Workbook\n",
        "\n",
        "This week we will look at Logistic regression and at model training using gradient descent.\n",
        "\n",
        "We will start by looking at fitting linear regression models with the same algorithm before extending it to binary outcomes."
      ],
      "metadata": {
        "id": "k89xlsM2pooY"
      },
      "id": "k89xlsM2pooY"
    },
    {
      "cell_type": "markdown",
      "id": "25f8ebaa",
      "metadata": {
        "id": "25f8ebaa"
      },
      "source": [
        "### Linear regression with one predictor"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this (made up) data set y is the exam results of a group of students and x is the number of hours spent studying."
      ],
      "metadata": {
        "id": "9CPkssfF_mpI"
      },
      "id": "9CPkssfF_mpI"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3d7882cd",
      "metadata": {
        "id": "3d7882cd"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import math\n",
        "\n",
        "x = [1.00,1.25,1.50,1.75,2.00,2.25,2.50,2.75,3.00,3.25,3.50,3.75,4.00]\n",
        "y = [33,49,41,54,52,45,36,58,45,69,55,56,68]\n",
        "x = np.array(x)\n",
        "y = np.array(y)\n",
        "\n",
        "plt.scatter(x, y)\n",
        "plt.ylim(0,100)\n",
        "plt.xlim(0,5)\n",
        "plt.scatter(x, y)\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In linear regression we fit a line to the data that allows us to predict y from x. The equation for this is as follows (note that I using the  machine-learning term bias in place of the term intercept which is more commonly used in statistics).\n",
        "\n",
        "y = bias + x*weight [+ error]\n",
        "\n",
        "Let's start by setting a random weight and setting our bias to zero.\n",
        "\n",
        "Note: Computers cannot actually generate random numbers, just numbers that appear random. So we would do better to consider the numbers generated to be pseudo-random. The algorithm deployed uses a \"seed\" during generation. If the seed is held constant then we will get the same \"random\" value each time. I set the seed here just so that everyone gets the same line and things don't get confusing. If I hadn't set this I would expect a different weight everytime I ran the code.\n"
      ],
      "metadata": {
        "id": "aO8EzOQ8DATd"
      },
      "id": "aO8EzOQ8DATd"
    },
    {
      "cell_type": "code",
      "source": [
        "np.random.seed(1)\n",
        "weight = np.random.rand(1)[0]\n",
        "bias=0\n",
        "weight"
      ],
      "metadata": {
        "id": "zDFRmccDD0KM"
      },
      "id": "zDFRmccDD0KM",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "683df270",
      "metadata": {
        "id": "683df270"
      },
      "outputs": [],
      "source": [
        "plt.scatter(x, y)\n",
        "plt.ylim(0,100)\n",
        "plt.xlim(0,5)\n",
        "line_x = [0, 5]\n",
        "line_y = [bias, bias+(5*weight)]\n",
        "\n",
        "plt.scatter(x, y)\n",
        "plt.plot(line_x, line_y, label='Line', color='red')  # Adding a line\n",
        "\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "As you can see this line isn't a very good fit to the data. If we used it for predictions we would get very bad estimates.\n",
        "\n",
        "We will use gradient descent to fine the line of best fit. Note: it is called simply gradient descent when we update weights based on the whole data set as we will here. If we update weights based on random subsets of the data (mini-batch training) it is called stochastic gradient descent. These variants are explained in the week 7 lecture."
      ],
      "metadata": {
        "id": "WQRRYvlEG51y"
      },
      "id": "WQRRYvlEG51y"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Gradient descent works as follows: \\\\\n",
        "Initialise weights to e.g. random values \\\\\n",
        "For a given number of N iterations:  \\\\\n",
        "1. Predict each y value given x, bias and the current weight(s), giving us y_hat \\\\\n",
        "2. Calculate the loss (for reporting/monitoring). For linear regression this is the \"mean squared error\". We calculate a vector of item-specific \"errors\" by subtracting the vector of real y values from the vector of estimated y_hat values. We then calculate the dot product of this vector with itself (the sum of the squared values) and divide that by the number of data points. \\\\\n",
        "3. Calculate dw - the gradient of the loss function with regard to each weight. For linear regression this is the dot product of the vector of x values for the given feature and the vector of errors, divided by the number of data points in our data. \\\\\n",
        "4. Calculate db - the gradient of the loss function with regard to the bias. For linear regression this is the sum of the vector of errors, divided by the number of data points in our data.  \\\\\n",
        "5. Update each weight (in this first example there is only one) by setting it to be the current weight minus dw times the learning rate \\\\\n",
        "6. Update the bias by setting it to be the current bias minus db times the learning rate \\\\\n",
        "7. Repeat until done\n"
      ],
      "metadata": {
        "id": "0LjBqRrmkrXx"
      },
      "id": "0LjBqRrmkrXx"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Problem 1: Complete the code below so that it finds the line of best fit"
      ],
      "metadata": {
        "id": "H2RcrtIVDISG"
      },
      "id": "H2RcrtIVDISG"
    },
    {
      "cell_type": "code",
      "source": [
        "n_iters = 250\n",
        "num_features=1\n",
        "weight = np.random.rand(num_features)[0]\n",
        "bias=0\n",
        "linear_loss=[]\n",
        "\n",
        "num_samples = len(y)\n",
        "lr=0.01\n",
        "\n",
        "for i in range(n_iters):\n",
        "    y_est = x*weight+bias\n",
        "    #print(y_est)\n",
        "    #Â´print(y)\n",
        "    errors = y_est-y\n",
        "    #print(errors)\n",
        "    loss = errors.dot(errors)/num_samples\n",
        "    linear_loss.append(loss)\n",
        "\n",
        "    dw = (1 / num_samples) * sum(x*errors)\n",
        "    db = (1 / num_samples) * sum(errors)\n",
        "    weight = weight - lr * dw\n",
        "    bias = bias - lr * db\n",
        "\n",
        "plt.plot(range(1,n_iters),linear_loss[1:])\n",
        "plt.xlabel(\"number of epochs\")\n",
        "plt.ylabel(\"loss\")"
      ],
      "metadata": {
        "id": "qyhjhn_hlV7J"
      },
      "id": "qyhjhn_hlV7J",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If we have done this correctly then when we use the bias and the weight to plot a line it should fit the data well"
      ],
      "metadata": {
        "id": "cfh6J7QFRZHM"
      },
      "id": "cfh6J7QFRZHM"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "06a5164c",
      "metadata": {
        "id": "06a5164c"
      },
      "outputs": [],
      "source": [
        "plt.scatter(x, y)\n",
        "plt.ylim(0,100)\n",
        "plt.xlim(0,5)\n",
        "line_x = [0, 5]\n",
        "line_y = [bias, bias+(5*weight)]\n",
        "\n",
        "plt.scatter(x, y)\n",
        "plt.plot(line_x, line_y, label='Line', color='red')  # Adding a line\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Problem 2: Increase the learning rate to 2 and rerun your code. What do you notice? Why does this happen? \\\\\n",
        "\n",
        "You should see that it doesn't find a solution. This is because the update to the bias and weight at each step is too extreme and the values jump around erratically never finding a solution"
      ],
      "metadata": {
        "id": "yAaIvbKIE_bt"
      },
      "id": "yAaIvbKIE_bt"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Problem 3. Decrease the learning rate to 0.001 and rerun your code. What do you notice? Why does this happen?\n",
        "\n",
        "The loss decreases at a slower rate. This is because each update is smaller and so each step down is smaller"
      ],
      "metadata": {
        "id": "y6a41WszFDqt"
      },
      "id": "y6a41WszFDqt"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Problem 4. What is the predicted exam score for a student who studied for a) 1 hour and b) 4 hours. To calculate this you will need to know the bias and the weight which are as follows:\n"
      ],
      "metadata": {
        "id": "IuEzG8GTBYgt"
      },
      "id": "IuEzG8GTBYgt"
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"BIAS: \" + str(bias))\n",
        "print(\"WEIGHT: \" + str(weight))"
      ],
      "metadata": {
        "id": "PghW0z1cSPZ2"
      },
      "id": "PghW0z1cSPZ2",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# a) student who studies for 1 hour\n",
        "print(bias + 1*weight)\n",
        "# a) student who studies for 4 hours\n",
        "print(bias + 4*weight)"
      ],
      "metadata": {
        "id": "idaSJBRZ0fjC"
      },
      "id": "idaSJBRZ0fjC",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "Yy7tq5Aw0ZT-"
      },
      "id": "Yy7tq5Aw0ZT-"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Linear Regression with 2 predictors\n",
        "\n",
        "Now imagine that we acquire an additional piece of information - each student's score on a recent mock exam. We now have two features to use to make our prediction. We can plot the relationship between the variables using a bubble plot."
      ],
      "metadata": {
        "id": "eg0gn4eJzEfb"
      },
      "id": "eg0gn4eJzEfb"
    },
    {
      "cell_type": "code",
      "source": [
        "x=[[41, 51, 35, 45, 52, 35, 31, 57, 45, 51, 60, 64, 63],[1.00,1.25,1.50,1.75,2.00,2.25,2.50,2.75,3.00,3.25,3.50,3.75,4.00]]\n",
        "y = [33,49,41,54,52,45,36,58,45,69,55,56,68]\n",
        "x = np.array(x)\n",
        "y = np.array(y)\n",
        "plt.scatter(x[0], x[1], s=np.exp(y/10), alpha=0.5)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "NL3dWZH93-Bz"
      },
      "id": "NL3dWZH93-Bz",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Before attempting to do anything with the data we will scale the predictors so that they have a mean of zero and a standard deviation of 1. This will aid learning as weights will be on manageable scales.\n"
      ],
      "metadata": {
        "id": "lebYsFJ3w11w"
      },
      "id": "lebYsFJ3w11w"
    },
    {
      "cell_type": "code",
      "source": [
        "x[0]=(x[0]-np.mean(x[0]))/np.std(x[0])\n",
        "x[1]=(x[1]-np.mean(x[1]))/np.std(x[1])"
      ],
      "metadata": {
        "id": "yM-GZEXExLS_"
      },
      "id": "yM-GZEXExLS_",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Problem 5: Complete the code below so that it finds the line of best fit. The critical difference is that we now have 2 features and therefore 2 weights to factor into our predictions and to update at each iteration"
      ],
      "metadata": {
        "id": "tNRtwWudlsJb"
      },
      "id": "tNRtwWudlsJb"
    },
    {
      "cell_type": "code",
      "source": [
        "n_iters = 2000\n",
        "num_features = 2\n",
        "weights = np.random.rand(num_features)\n",
        "bias=0\n",
        "num_samples = len(y)\n",
        "linear_loss=[]\n",
        "lr=0.025\n",
        "for i in range(n_iters):\n",
        "    y_est = x[0]*weights[0]+x[1]*weights[1]+bias\n",
        "    errors = y_est-y\n",
        "    loss = errors.dot(errors)/num_samples\n",
        "    linear_loss.append(loss)\n",
        "\n",
        "    dw1 = (1 / num_samples) * sum(x[0]*errors)\n",
        "    dw2 = (1 / num_samples) * sum(x[1]*errors)\n",
        "    db = (1 / num_samples) * sum(errors)\n",
        "    weights[0] = weights[0] - lr * dw1\n",
        "    weights[1] = weights[1] - lr * dw2\n",
        "    bias = bias - lr * db\n",
        "plt.plot(range(1,n_iters),linear_loss[1:])\n",
        "plt.xlabel(\"number of epochs\")\n",
        "plt.ylabel(\"loss\")"
      ],
      "metadata": {
        "id": "eRzZhdcWlvl5"
      },
      "id": "eRzZhdcWlvl5",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "See week 7 worksheet for coverage of logistic regression"
      ],
      "metadata": {
        "id": "pShqLsHczutF"
      },
      "id": "pShqLsHczutF"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.9"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "61td5eSnl5aI"
      },
      "source": [
        "# Week 12 Seminar 2 Notebook: Training a Softmax Classifier\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mWv8pOTKf1YA"
      },
      "source": [
        "Problem 1: Given the following weights and input for a 3-class classifier perform the forward pass to generate the probability for each class.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n86usvs_fX9Y"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "weights=np.array([[0.77, 0.63, 0.5],[0.02, 0.74, 0.22],[0.2, 0.76, 0.17]])\n",
        "x=np.array([2.33,3.72,1.0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bgwScD13aujo"
      },
      "outputs": [],
      "source": [
        "z=x.dot(weights)\n",
        "q=np.exp(z)/np.exp(z).sum()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SL4cSPdjiMBN"
      },
      "source": [
        "Problem 2: With a true label of class 0 (from 0, 1 and 2) and the initial weights and inputs above perform the backwards pass and update the weights. The result should match that seen in this week's lecture."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mJCN8oEXcO_e"
      },
      "outputs": [],
      "source": [
        "# A solution is as follows\n",
        "y=[1,0,0]\n",
        "lr=0.2\n",
        "dw=np.array([(q-y)*a for a in x])\n",
        "#dw=np.outer((q.T-y),x).T\n",
        "weights=weights-lr*dw\n",
        "weights"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gzkxuED4Au40"
      },
      "source": [
        "Now we generate a full dataset for softmax classification"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rfJwFw6wBXTp"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "## Create simulated data\n",
        "np.random.seed(10)\n",
        "w1_center = (1, 3)\n",
        "w2_center = (3, 1)\n",
        "w3_center = (1, 1)\n",
        "w4_center = (3, 3)\n",
        "\n",
        "x=np.concatenate((np.random.normal(loc=w1_center,size=(20,2)),np.random.normal(loc=w2_center,size=(20,2)),np.random.normal(loc=w3_center,size=(10,2)),np.random.normal(loc=w4_center,size=(10,2))))\n",
        "\n",
        "labs=np.repeat([0,1,2],[20,20,20],axis=0)\n",
        "y=np.repeat(np.diag((1,1,1)),[20,20,20],axis=0)\n",
        "x=x.T\n",
        "x=np.array([x[0],x[1],[1] * len(x[0])]).T"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PwC7MNBqA_98"
      },
      "source": [
        "Problem 3. Given the data generated above write the code to train a multiclass classifier to convergence."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Gu48mcjOPHKo"
      },
      "outputs": [],
      "source": [
        "# A solution is as follows\n",
        "\n",
        "np.random.seed(10)\n",
        "n_iters = 1000\n",
        "num_features=3\n",
        "num_classes=3\n",
        "num_samples = len(y)\n",
        "weights = np.random.rand(num_classes,num_features)\n",
        "lr=0.2\n",
        "logistic_loss=[]\n",
        "z=np.zeros((num_samples,num_classes))\n",
        "q=np.zeros((num_samples,num_classes))\n",
        "\n",
        "for i in range(n_iters):\n",
        "\n",
        "    z= x.dot(weights)\n",
        "    z_sum=np.exp(z).sum(axis=1)\n",
        "    q=np.array([list(np.exp(z_i)/z_sum[i]) for i, z_i in enumerate(z)])\n",
        "    #print(q)\n",
        "    loss=np.mean(-np.log2((np.sum((y*q),axis=1))))\n",
        "\n",
        "    logistic_loss.append(loss)\n",
        "\n",
        "    dw=x.T.dot((q-y))/num_samples\n",
        "    weights=(weights - (dw*lr))\n",
        "\n",
        "plt.plot(range(1,n_iters),logistic_loss[1:])\n",
        "plt.xlabel(\"number of epochs\")\n",
        "plt.ylabel(\"loss\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KeKQq_efrbOI"
      },
      "outputs": [],
      "source": [
        "# Accuracy\n",
        "np.mean(np.argmax(y,axis=1) == np.argmax(q,axis=1))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m3nRLvrCXucO"
      },
      "source": [
        "Problem 4.  Write code to calculate the multiclass precision and recall using macroaveraging (see week 7 lectures slides pt 2 for a definition). The code for calculating precision and recall for binary classifiers from week 8 will provide a useful starting point."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "z = x.dot(weights)\n",
        "z_sum=np.exp(z).sum(axis=1)\n",
        "q=np.array([list(np.exp(z_i)/z_sum[i]) for i, z_i in enumerate(z)])\n",
        "y_test_pred=np.argmax(q,axis=1)\n",
        "y_test_true=np.argmax(y,axis=1)"
      ],
      "metadata": {
        "id": "fNJp9CGyN9vH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "TP_0 = sum(np.array([int(s == 0 and y_test_true[i] == 0) for i,s in enumerate(y_test_pred)]))\n",
        "FP_0 = sum(np.array([int(s == 0 and y_test_true[i] != 0) for i,s in enumerate(y_test_pred)]))\n",
        "FN_0 = sum(np.array([int(s != 0 and y_test_true[i] == 0) for i,s in enumerate(y_test_pred)]))\n",
        "\n",
        "TP_1 = sum(np.array([int(s == 1 and y_test_true[i] == 1) for i,s in enumerate(y_test_pred)]))\n",
        "FP_1 = sum(np.array([int(s == 1 and y_test_true[i] != 1) for i,s in enumerate(y_test_pred)]))\n",
        "FN_1 = sum(np.array([int(s != 1 and y_test_true[i] == 1) for i,s in enumerate(y_test_pred)]))\n",
        "\n",
        "TP_2 = sum(np.array([int(s == 2 and y_test_true[i] == 2) for i,s in enumerate(y_test_pred)]))\n",
        "FP_2 = sum(np.array([int(s == 2 and y_test_true[i] != 2) for i,s in enumerate(y_test_pred)]))\n",
        "FN_2 = sum(np.array([int(s != 2 and y_test_true[i] == 2) for i,s in enumerate(y_test_pred)]))\n",
        "\n",
        "PRECISION_0 = TP_0 / (TP_0+FP_0)\n",
        "PRECISION_1 = TP_1 / (TP_1+FP_1)\n",
        "PRECISION_2 = TP_2 / (TP_2+FP_2)\n",
        "\n",
        "PRECISION_MACROAVERAGE = (PRECISION_0+PRECISION_1+PRECISION_2)/3\n",
        "print(PRECISION_MACROAVERAGE)\n",
        "\n",
        "RECALL_0 = TP_0 / (TP_0+FN_0)\n",
        "RECALL_1 = TP_1 / (TP_1+FN_1)\n",
        "RECALL_2 = TP_2 / (TP_2+FN_2)\n",
        "\n",
        "RECALL_MACROAVERAGE = (RECALL_0+RECALL_1+RECALL_2)/3\n",
        "print(RECALL_MACROAVERAGE)\n"
      ],
      "metadata": {
        "id": "f0nWfMKwN4Fz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nsJveI-yYg4R"
      },
      "source": [
        "## Building an intent classifier\n",
        "\n",
        "Below you can find data for the task of intent classification. This is an important step in modern task-based dialogue systems - given a particular piece of input from the speaker, the system tries to determine what goal the speaker is trying to achieve, in order that it can then produce an appropriate response.\n",
        "\n",
        "The data set pairs 2100 reviews with one of seven different intents:\n",
        "\n",
        "'PlayMusic', e.g. \"play easy listening\" \\\\\n",
        "'AddToPlaylist' e.g. \"please add this song to road trip\" \\\\\n",
        "'RateBook' e.g. \"give this novel 5 stars\" \\\\\n",
        "'SearchScreeningEvent' e.g. \"give me a list of local movie times\" \\\\\n",
        "'BookRestaurant' e.g. \"i'd like a table for four at 7pm at Asti\" \\\\\n",
        "'GetWeather' e.g. \"what's it like outside\" \\\\\n",
        "'SearchCreativeWork' \"show me the new James Bond trailer\" \\\\\n",
        "\n",
        "To import the data please run the following cells.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yqznaPDmMgoS"
      },
      "outputs": [],
      "source": [
        "! wget https://raw.githubusercontent.com/cbannard/lela60331_24-25/refs/heads/main/intent_classification.csv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6eSkDiJhM7Y9"
      },
      "outputs": [],
      "source": [
        "# Import the data into utterances and labels lists\n",
        "utterances=[]\n",
        "labels=[]\n",
        "\n",
        "with open(\"intent_classification.csv\") as f:\n",
        "   # iterate over the lines in the file\n",
        "   for line in f.readlines()[1:]:\n",
        "        # split the current line into a list of two element - the review and the label\n",
        "        fields = line.rstrip().split(',')\n",
        "        # put the current review in the reviews list\n",
        "        utterances.append(fields[0])\n",
        "        # put the current sentiment rating in the labels list\n",
        "        labels.append(fields[1])\n",
        "## Select vocabulary for inclusion in one-hot representations\n",
        "from collections import Counter\n",
        "import re\n",
        "# Tokenise the text, turning a list of strings into a list of lists of tokens. We use very naive space-based tokenisation.\n",
        "tokenized_sents = [re.findall(\"[^ ]+\",txt) for txt in utterances]\n",
        "# Collapse all tokens into a single list\n",
        "tokens=[]\n",
        "for s in tokenized_sents:\n",
        "      tokens.extend(s)\n",
        "# Count the tokens in the tokens list. The returns a list of tuples of each token and count\n",
        "counts=Counter(tokens)\n",
        "# Sort the tuples. The reverse argument instructs to put most frequent first rather than last (which is the default)\n",
        "so=sorted(counts.items(), key=lambda item: item[1], reverse=True)\n",
        "# Extract the list of tokens, by transposing the list of lists so that there is a list of tokens a list of counts and then just selecting the former\n",
        "so=list(zip(*so))[0]\n",
        "# Select the firs 5000 words in the list\n",
        "type_list=so[0:2500]\n",
        "\n",
        "# Create a 2100 x 2500 matrix of zeros\n",
        "M = np.zeros((len(utterances), len(type_list)))\n",
        "#iterate over the reviews\n",
        "for i, utt in enumerate(utterances):\n",
        "    # Tokenise the current review:\n",
        "    tokens = re.findall(\"[^ ]+\",utt)\n",
        "    # iterate over the words in our type list (the set of 5000 words):\n",
        "    for j,t in enumerate(type_list):\n",
        "        # if the current word j occurs in the current review i then set the matrix element at i,j to be one. Otherwise leave as zero.\n",
        "        if t in tokens:\n",
        "              M[i,j] = 1\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RH1JA9zGN0Ov"
      },
      "outputs": [],
      "source": [
        "train_ints=np.random.choice(len(utterances),int(len(utterances)*0.8),replace=False)\n",
        "remaining_ints=list(set(range(0,len(utterances))) - set(train_ints))\n",
        "test_ints=np.random.choice(len(remaining_ints),int(len(remaining_ints)*0.5),replace=False)\n",
        "dev_ints=list(set(range(0,len(remaining_ints))) - set(test_ints))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i9Za2xE8ODu9"
      },
      "outputs": [],
      "source": [
        "\n",
        "### Generate one-hot encoded input matrices for training, development and test\n",
        "M_train = np.array(M[train_ints,]).T\n",
        "M_test = np.array(M[test_ints,]).T\n",
        "M_dev = np.array(M[dev_ints,]).T\n",
        "\n",
        "unique_labels=list(set(labels))\n",
        "unique_one_hot=np.diag(np.ones(len(unique_labels)))\n",
        "\n",
        "labels_train = [labels[i] for i in train_ints]\n",
        "labels_test = [labels[i] for i in test_ints]\n",
        "labels_dev = [labels[i] for i in dev_ints]\n",
        "\n",
        "### Generate one-hot encoded target output matrices for training, development and test\n",
        "y_train=np.array([list(unique_one_hot[k]) for k in [unique_labels.index(x) for x in labels_train]]).T\n",
        "y_test=np.array([list(unique_one_hot[k]) for k in [unique_labels.index(x) for x in labels_test]]).T\n",
        "y_dev=np.array([list(unique_one_hot[k]) for k in [unique_labels.index(x) for x in labels_dev]]).T\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4zFDhD0iZ7Wa"
      },
      "source": [
        "The data has been split into training set of 1680, a development set of 210 and a test set of 210."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OlfEjSUlaHs0"
      },
      "source": [
        "Problem 5. Train a softmax classifier for this task on the training set. You can test performance on the development set as you go."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "M_train=M_train.T\n",
        "M_test=M_test.T\n",
        "M_dev=M_dev.T\n",
        "y_train=y_train.T\n",
        "y_test=y_test.T\n",
        "y_dev=y_dev.T"
      ],
      "metadata": {
        "id": "8sH12H29S6hl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "np.random.seed(10)\n",
        "n_iters = 50\n",
        "num_features=2500\n",
        "num_classes=7\n",
        "num_samples = len(y)\n",
        "weights = np.random.rand(num_features,num_classes)\n",
        "lr=0.2\n",
        "logistic_loss=[]\n",
        "z=np.zeros((num_samples,num_classes))\n",
        "q=np.zeros((num_samples,num_classes))\n",
        "\n",
        "for i in range(n_iters):\n",
        "\n",
        "    z= M_train.dot(weights)\n",
        "    z_sum=np.exp(z).sum(axis=1)\n",
        "    q=np.array([list(np.exp(z_i)/z_sum[i]) for i, z_i in enumerate(z)])\n",
        "    #print(q)\n",
        "    loss=np.mean(-np.log2((np.sum((y_train*q),axis=1))))\n",
        "\n",
        "    logistic_loss.append(loss)\n",
        "\n",
        "    dw=M_train.T.dot((q-y_train))/num_samples\n",
        "    weights=(weights - (dw*lr))\n",
        "\n",
        "plt.plot(range(1,n_iters),logistic_loss[1:])\n",
        "plt.xlabel(\"number of epochs\")\n",
        "plt.ylabel(\"loss\")"
      ],
      "metadata": {
        "id": "plwgg5_3S2MN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qa6-F7a5achg"
      },
      "source": [
        "Problem 6. Evaluate your classifier, by calculating precision and recall on the test set."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "z = M_test.dot(weights)\n",
        "z_sum=np.exp(z).sum(axis=1)\n",
        "q=np.array([list(np.exp(z_i)/z_sum[i]) for i, z_i in enumerate(z)])\n",
        "y_test_pred=np.argmax(q,axis=1)\n",
        "y_test_true=np.argmax(y_test,axis=1)\n",
        "\n",
        "TP=[]\n",
        "for j in range(7):\n",
        " TP.append(np.sum(np.array([int(s == j and y_test_true[i] == j) for i,s in enumerate(y_test_pred)])))\n",
        "\n",
        "FP=[]\n",
        "for j in range(7):\n",
        " FP.append(np.sum(np.array([int(s == j and y_test_true[i] != j) for i,s in enumerate(y_test_pred)])))\n",
        "\n",
        "FN=[]\n",
        "for j in range(7):\n",
        " FN.append(np.sum(np.array([int(s != j and y_test_true[i] == j) for i,s in enumerate(y_test_pred)])))\n",
        "\n",
        "\n",
        "PRECISION=np.array(TP)/(np.array(TP)+np.array(FP))\n",
        "RECALL=np.array(TP)/(np.array(TP)+np.array(FN))\n",
        "\n",
        "PRECISION_MACROAVERAGE=np.mean(PRECISION)\n",
        "print(PRECISION_MACROAVERAGE)\n",
        "\n",
        "RECALL_MACROAVERAGE=np.mean(RECALL)\n",
        "print(RECALL_MACROAVERAGE)\n"
      ],
      "metadata": {
        "id": "xNBZnZSBTk9K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zjv8cK2MzqzR"
      },
      "source": [
        "### Batch training\n",
        "\n",
        "When training classifiers we can update the weights on different schedules - once for datapoint, once for every pass through the full dataset, or once for every pass through each of K randomly sampled batch of size N. First we need to decide on K and N. Then we generate our batches as in the next cell."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IG0RmsnFueJk"
      },
      "outputs": [],
      "source": [
        "k = 10\n",
        "# Create array of all indices in training data\n",
        "a=np.arange(y_train.shape[0])\n",
        "# randomly shuffle indices in place (in case of classes being unequally distributed across positions in data)\n",
        "np.random.shuffle(a)\n",
        "# Split indices into k equal batches\n",
        "batches=np.array(np.split(a, k))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kaYkvhtt0wsR"
      },
      "source": [
        "This gives us 10 batches of 168 randomly sampled indices that can then be used to select the data in batches at training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u3W456WByNHw"
      },
      "outputs": [],
      "source": [
        "batches.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Jfdp6kQSLh-"
      },
      "source": [
        "Batches (e.g. batch 4) can be selected as follows"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NN8k5gMESKO3"
      },
      "outputs": [],
      "source": [
        "j=4\n",
        "this_batch_M_train = M_train.T[batches[j]]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "M_train[batches[j]].shape"
      ],
      "metadata": {
        "id": "1U3agCtUWs7E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hcd2NsI-Sarp"
      },
      "source": [
        "Problem 7. Rewrite your intent classifier code from above so that it iterates through the batches and updates the parameters after each batch."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "np.random.seed(10)\n",
        "n_iters = 50\n",
        "num_features=2500\n",
        "num_classes=7\n",
        "num_samples = len(y)\n",
        "weights = np.random.rand(num_features,num_classes)\n",
        "lr=0.2\n",
        "logistic_loss=[]\n",
        "z=np.zeros((num_samples,num_classes))\n",
        "q=np.zeros((num_samples,num_classes))\n",
        "\n",
        "for i in range(n_iters):\n",
        "    loss=0.0\n",
        "    for j in range(len(batches)):\n",
        "      this_batch_M_train = M_train[batches[j]]\n",
        "      this_batch_y_train = y_train[batches[j]]\n",
        "      z= this_batch_M_train.dot(weights)\n",
        "      z_sum=np.exp(z).sum(axis=1)\n",
        "      q=np.array([list(np.exp(z_i)/z_sum[i]) for i, z_i in enumerate(z)])\n",
        "      #print(q)\n",
        "      loss+=np.mean(-np.log2((np.sum((this_batch_y_train*q),axis=1))))\n",
        "\n",
        "      dw=this_batch_M_train.T.dot((q-this_batch_y_train))\n",
        "      weights=(weights - (dw*lr))\n",
        "    logistic_loss.append(loss)\n",
        "plt.plot(range(1,n_iters),logistic_loss[1:])\n",
        "plt.xlabel(\"number of epochs\")\n",
        "plt.ylabel(\"loss\")"
      ],
      "metadata": {
        "id": "WT-MoOVEWH2p"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d3e70908"
      },
      "source": [
        "# Week 7 Seminar: Logistic Regression\n",
        "\n",
        "Today we are going to apply what we have learned to logistic regression, first with some invented data and then with some real text\n",
        "\n",
        "First we will generate some random data for an imagined sentiment classification task with only two features. We can think of our two features as being the log of the counts of positive words (e.g. good, excellent) and the log of the counts of negative words (e.g. bad, rubbish). The label we are trying to predict is either 1 (positive sentiment text) or 0 (negative sentiment text)."
      ],
      "id": "d3e70908"
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "RRsFYQMuzbvB"
      },
      "id": "RRsFYQMuzbvB",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "75fd7147"
      },
      "outputs": [],
      "source": [
        "## Create simulated data\n",
        "np.random.seed(10)\n",
        "w1_center = (2, 3)\n",
        "w2_center = (3, 2)\n",
        "batch_size=50\n",
        "\n",
        "x = np.zeros((batch_size, 2))\n",
        "y = np.zeros(batch_size)\n",
        "for i in range(batch_size):\n",
        "    if np.random.random() > 0.5:\n",
        "        x[i] = np.random.normal(loc=w1_center)\n",
        "    else:\n",
        "        x[i] = np.random.normal(loc=w2_center)\n",
        "        y[i] = 1\n",
        "\n",
        "x=x.T"
      ],
      "id": "75fd7147"
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can visualise the data as follows. The stars are the positive sentiment texts, the circles are the negative sentiment texts."
      ],
      "metadata": {
        "id": "hrt8YVTM4rb2"
      },
      "id": "hrt8YVTM4rb2"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "32bd6db8"
      },
      "outputs": [],
      "source": [
        "plt.scatter(x[0][y==0], x[1][y==0], marker='*', s=100)\n",
        "plt.scatter(x[0][y==1], x[1][y==1], marker='o', s=100)\n",
        "plt.xlabel(\"log count of negative words\")\n",
        "plt.ylabel(\"log count of positive words\")\n",
        "plt.xlim((0,5))\n",
        "plt.ylim((0,5))\n"
      ],
      "id": "32bd6db8"
    },
    {
      "cell_type": "markdown",
      "source": [
        "To see why we might to take the log, we can exponentiate the log counts (reversing the log function) to give raw counts. These are worse for visualisation and modelling purposes"
      ],
      "metadata": {
        "id": "T-bHfx_w42y3"
      },
      "id": "T-bHfx_w42y3"
    },
    {
      "cell_type": "code",
      "source": [
        "x_exp=np.exp(x)\n",
        "plt.scatter(x_exp[0][y==0], x_exp[1][y==0], marker='*', s=100)\n",
        "plt.scatter(x_exp[0][y==1], x_exp[1][y==1], marker='o', s=100)\n",
        "plt.xlabel(\"count of negative words\")\n",
        "plt.ylabel(\"count of positive words\")\n",
        "plt.xlim((0,150))\n",
        "plt.ylim((0,150))"
      ],
      "metadata": {
        "id": "f35tC8HF0k-r"
      },
      "execution_count": null,
      "outputs": [],
      "id": "f35tC8HF0k-r"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Our goal in logistic regression is to find a line that allows us to estimate a probability that any text has positive sentiment. It that probability is greater than 0.5 then we will say that it is a positive text and if lower then we will say it is a negative text.\n",
        "\n",
        "In logistic regression we first estimate a value z as a linear function of our predictors, just as in linear regression:\n",
        "\n",
        "$y_{i} = bias + x_{i}*weight$\n",
        "\n",
        "We then use the sigmoid function to convert this z values to a probability:\n",
        "\n",
        "$p(y_{i}=1) = \\frac{1}{1+e^{-z}}$\n",
        "\n",
        "\n",
        "We can start by setting some random weights and an arbitrary bias."
      ],
      "metadata": {
        "id": "4O3GYaLW5TXk"
      },
      "id": "4O3GYaLW5TXk"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b1078b2a"
      },
      "outputs": [],
      "source": [
        "np.random.seed(10)\n",
        "num_features=2\n",
        "weights = np.random.rand(num_features)\n",
        "bias=0"
      ],
      "id": "b1078b2a"
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can add this line to our plot of values. It should cut across the items so that items that are above the line should be mostly positive sentiment texts and those that are below should be negative sentiment texts."
      ],
      "metadata": {
        "id": "a-y6Y6m96klo"
      },
      "id": "a-y6Y6m96klo"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a287cb21"
      },
      "outputs": [],
      "source": [
        "plt.scatter(x[0][y==0], x[1][y==0], marker='*', s=100)\n",
        "plt.scatter(x[0][y==1], x[1][y==1], marker='o', s=100)\n",
        "plt.xlim((-5,5))\n",
        "plt.ylim((-5,5))\n",
        "c = -bias/weights[1]\n",
        "m = -weights[0]/weights[1]\n",
        "xmin, xmax = 0, 5\n",
        "ymin, ymax = 0, 5\n",
        "xd = np.array([xmin, xmax])\n",
        "yd = m*xd + c\n",
        "plt.plot(xd, yd, 'k', lw=1, ls='--')"
      ],
      "id": "a287cb21"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Our random line does not do this. So we will use gradient descent to find the line of best fit.\n",
        "\n",
        "For logistic regression we use a cross entropy loss function. I have included this in the code (See lecture for details).\n",
        "\n",
        "To calculate the gradient of the loss function with respect to the bias term we first calculate the difference between each predicted y value and the true y value. We then take the average difference by summing the differences and dividing the result by N - the number of data points in our data:\n",
        "\n",
        "$\\delta b = \\frac{1}{N} * \\sum_{i=1}^{N} q_{i} - y_{i} $\n",
        "\n",
        "To calculate the gradient of the loss function with respect to each weight, we again first calculate the difference between each predicted y value and the true y value. We then calculate the dot product of this vector and the vector of x values for the relevant feature and divide the result by N - the number of data points in our data:\n",
        "\n",
        "$\\delta w = \\frac{1}{N} * \\sum_{i=1}^{N} x[i] * q_{i} - y_{i} $\n",
        "\n",
        "x here is vector of values for the feature relevant to the individual weight. A different gradient is needed for each weight and this will be calculated using a different x.\n"
      ],
      "metadata": {
        "id": "dskOZYTy6_8N"
      },
      "id": "dskOZYTy6_8N"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Problem 1: Complete code below so that it finds the line of best fit. \\\\\n",
        "\n",
        "Note: For the sigmoid function you will need to exponentiate -z. You can do this using the function np.exp(-z)."
      ],
      "metadata": {
        "id": "gxnf8Ob5mWyb"
      },
      "id": "gxnf8Ob5mWyb"
    },
    {
      "cell_type": "code",
      "source": [
        "np.random.seed(10)\n",
        "num_features=2\n",
        "weights = np.random.rand(num_features)\n",
        "bias=0\n",
        "\n",
        "n_iters = 2500\n",
        "num_features = 2\n",
        "num_samples = len(y)\n",
        "lr=0.001\n",
        "logistic_loss=[]\n",
        "\n",
        "for i in range(n_iters):\n",
        "    z=????\n",
        "    q = ????\n",
        "    loss = sum(-(y*np.log2(q)+(1-y)*np.log2(1-q)))\n",
        "    logistic_loss.append(loss)\n",
        "    dw1 = ?????\n",
        "    dw2 = ?????\n",
        "    db = ?????\n",
        "    weights[0] = ?????\n",
        "    weights[1] = ?????\n",
        "    bias = ?????\n",
        "plt.plot(range(1,n_iters),logistic_loss[1:])\n",
        "plt.xlabel(\"number of epochs\")\n",
        "plt.ylabel(\"loss\")"
      ],
      "metadata": {
        "id": "m62X7TZn5gGi"
      },
      "execution_count": null,
      "outputs": [],
      "id": "m62X7TZn5gGi"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Once this is working we can add the resulting line to our data and it should separate the two classes of items."
      ],
      "metadata": {
        "id": "mkZ8zmaBDMBQ"
      },
      "id": "mkZ8zmaBDMBQ"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1414def9"
      },
      "outputs": [],
      "source": [
        "plt.scatter(x[0][y==0], x[1][y==0], marker='*', s=100)\n",
        "plt.scatter(x[0][y==1], x[1][y==1], marker='o', s=100)\n",
        "plt.xlim((-5,5))\n",
        "plt.ylim((-5,5))\n",
        "c = -bias/weights[1]\n",
        "m = -weights[0]/weights[1]\n",
        "xmin, xmax = 0, 5\n",
        "ymin, ymax = -5, 5\n",
        "xd = np.array([xmin, xmax])\n",
        "yd = m*xd + c\n",
        "plt.plot(xd, yd, 'k', lw=1, ls='--')"
      ],
      "id": "1414def9"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Problem 2: Calculate p(y=1) for a) a text that contains two positive words and 3 negative words and b) a text that contains 10 positive words and 1 negative word.\n",
        "\n",
        "To calculate this you will need to know the bias and the weight which are as follows. You will also need to use the sigmoid function."
      ],
      "metadata": {
        "id": "cZ1qxyrlDd6_"
      },
      "id": "cZ1qxyrlDd6_"
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"BIAS: \" + str(bias))\n",
        "print(\"WEIGHT 1: \" + str(weights[0]))\n",
        "print(\"WEIGHT 2: \" + str(weights[1]))"
      ],
      "metadata": {
        "id": "f2MMXcb8D1hS"
      },
      "execution_count": null,
      "outputs": [],
      "id": "f2MMXcb8D1hS"
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Matric-Vector multiplication\n",
        "\n",
        "In our model building so far we have been generating our predicted values by taking the dot product of vectors of weights and vectors of features (and adding the bias). However we want to move on to using more features than just 2. When we have lots of features, it is more efficient to handle the sets of j features for k items as a k x j matrix rather than as j vectors of length k. To combine our weights and our features we then need to take the dot product of each row of our feature matrix with our vector of weights. As you saw in this week's lecture we can take the dot product between a matrix with j columns and a row vector of length j. This can be done in numpy as follows:"
      ],
      "metadata": {
        "id": "Mbngjp6j8WJF"
      },
      "id": "Mbngjp6j8WJF"
    },
    {
      "cell_type": "code",
      "source": [
        "vector = np.random.rand(3,1)\n",
        "matrix = np.random.rand(10,3)\n",
        "matrix.dot(vector) # or matrix@vector"
      ],
      "metadata": {
        "id": "o6Ku_XzQkrDq"
      },
      "id": "o6Ku_XzQkrDq",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Remember that in matrix multiplication order is important. The matrix here cannot be left multiplied by the vector because its number of rows does not equal the number of elements in the vector. And so the following doesn't work:"
      ],
      "metadata": {
        "id": "8XEQjqjfymfR"
      },
      "id": "8XEQjqjfymfR"
    },
    {
      "cell_type": "code",
      "source": [
        "vector.dot(matrix)"
      ],
      "metadata": {
        "id": "qowx2--kvd9q"
      },
      "id": "qowx2--kvd9q",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Logistic regression with text: sentiment analysis\n",
        "\n",
        "The dataset we are going to use here is 10000 reviews on Yelp classified as negative (1 or 2 star) or positive (3 or 4 star). We are going to train a classifier using this a part of this data and test its performance on another part.\n",
        "\n",
        "First we download the dataset:"
      ],
      "metadata": {
        "id": "OC5U-WIK5AzZ"
      },
      "id": "OC5U-WIK5AzZ"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1d0fbf90",
      "metadata": {
        "id": "1d0fbf90"
      },
      "outputs": [],
      "source": [
        "!wget https://raw.githubusercontent.com/cbannard/lela60331_24-25/refs/heads/main/data/yelp_reviews.txt"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The dataset is in a tab-delimited file, so that the first element on each row is a review and then there is a tab and then the sentiment rating.\n",
        "\n",
        "We read it into to lists (one for reviews; one for labels) as follows:"
      ],
      "metadata": {
        "id": "XYB44ZXbRG4G"
      },
      "id": "XYB44ZXbRG4G"
    },
    {
      "cell_type": "code",
      "source": [
        "# Create lists\n",
        "reviews=[]\n",
        "labels=[]\n",
        "\n",
        "with open(\"yelp_reviews.txt\") as f:\n",
        "   # iterate over the lines in the file\n",
        "   for line in f.readlines()[1:]:\n",
        "        # split the current line into a list of two element - the review and the label\n",
        "        fields = line.rstrip().split('\\t')\n",
        "        # put the current review in the reviews list\n",
        "        reviews.append(fields[0])\n",
        "        # put the current sentiment rating in the labels list\n",
        "        labels.append(fields[1])\n",
        "\n"
      ],
      "metadata": {
        "id": "KMbHKlsjz1wH"
      },
      "id": "KMbHKlsjz1wH",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We are going to represent our data using one-hot encoding. We need to use the same vocabulary for our training and test data so we do this prior to splitting the data.\n",
        "\n",
        "In order to one-hot encode we need to create a list of the included vocabulary items. We will use the 5000 most frequent words (not an ideal solution but a convenient one). To get this list we extract all the words from all the reviews, count how often they occur, sort them and then take the most frequent 5000 words. To count the words we are going to use the Counter object from the collections module."
      ],
      "metadata": {
        "id": "0hqHqxKGSJwb"
      },
      "id": "0hqHqxKGSJwb"
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter\n",
        "import re\n",
        "# Tokenise the text, turning a list of strings into a list of lists of tokens. We use very naive space-based tokenisation.\n",
        "tokenized_sents = [re.findall(\"[^ ]+\",txt) for txt in reviews]\n",
        "# Collapse all tokens into a single list\n",
        "tokens=[]\n",
        "for s in tokenized_sents:\n",
        "      tokens.extend(s)\n",
        "# Count the tokens in the tokens list. The returns a list of tuples of each token and count\n",
        "counts=collections.Counter(tokens)\n",
        "# Sort the tuples. The reverse argument instructs to put most frequent first rather than last (which is the default)\n",
        "so=sorted(counts.items(), key=lambda item: item[1], reverse=True)\n",
        "# Extract the list of tokens, by transposing the list of lists so that there is a list of tokens a list of counts and then just selecting the former\n",
        "so=list(zip(*so))[0]\n",
        "# Select the firs 5000 words in the list\n",
        "type_list=so[0:5000]"
      ],
      "metadata": {
        "id": "rXVFg3H442fc"
      },
      "id": "rXVFg3H442fc",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We are now ready to one-hot encode our reviews. We have 10000 reviews and a selected vocabulary of 5000 words. We therefore want to end up with 10000 x 5000 matrix **M**, where each row $i$ is a review, each column $j$ is a unique word from the vocab, and each element $x_{i,j}$ is a one if the word j occurs in review i and a zero otherwise."
      ],
      "metadata": {
        "id": "E5SAuA6QaZXf"
      },
      "id": "E5SAuA6QaZXf"
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a 10000 x 5000 matrix of zeros\n",
        "M = np.zeros((len(sents), len(type_list)))\n",
        "#iterate over the reviews\n",
        "for i, rev in enumerate(reviews):\n",
        "    # Tokenise the current review:\n",
        "    tokens = re.findall(\"[^ ]+\",rev)\n",
        "    # iterate over the words in our type list (the set of 5000 words):\n",
        "    for j,t in enumerate(type_list):\n",
        "        # if the current word j occurs in the current review i then set the matrix element at i,j to be one. Otherwise leave as zero.\n",
        "        if t in tokens:\n",
        "              M[i,j] = 1\n",
        "\n"
      ],
      "metadata": {
        "id": "5HgWE38SMIFk"
      },
      "id": "5HgWE38SMIFk",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we are ready to split our data. We are going to use 20% of our data as test items, so we randomly select 8000 indices between 0 and 10000, which are the indices of our training items. The remaining 2000 indices are the indices of our test items.\n",
        "\n",
        "In a real development task we would want to split data into training, development and test. Here we just use training and test."
      ],
      "metadata": {
        "id": "ccEkUhsheHR6"
      },
      "id": "ccEkUhsheHR6"
    },
    {
      "cell_type": "code",
      "source": [
        "train_ints=np.random.choice(len(reviews),int(len(reviews)*0.8),replace=False)\n",
        "test_ints=list(set(range(0,len(reviews))) - set(train_ints))"
      ],
      "metadata": {
        "id": "JjTsjjDfCa5p"
      },
      "id": "JjTsjjDfCa5p",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We next use the indices to select the rows of our one-hot-encoded matrix M that correspond to our training items and our test items and put these into two separate matrices. We also select the corresponding labels."
      ],
      "metadata": {
        "id": "cDyfEsoRevxE"
      },
      "id": "cDyfEsoRevxE"
    },
    {
      "cell_type": "code",
      "source": [
        "M_train = M[train_ints,]\n",
        "M_test = M[test_ints,]\n",
        "labels_train = [labels[i] for i in train_ints]\n",
        "labels_test = [labels[i] for i in test_ints]"
      ],
      "metadata": {
        "id": "7GzcI2BeDWap"
      },
      "id": "7GzcI2BeDWap",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we are ready to train our model using the training data."
      ],
      "metadata": {
        "id": "Keub330AfSJv"
      },
      "id": "Keub330AfSJv"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Problem 3 Complete the code below so that it learns a logistic regression model from the training data."
      ],
      "metadata": {
        "id": "mN_odk43faPo"
      },
      "id": "mN_odk43faPo"
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "\n",
        "num_features=5000\n",
        "y=[int(l == \"positive\") for l in labels_train]\n",
        "weights = np.random.rand(num_features)\n",
        "bias=np.random.rand(1)\n",
        "n_iters = 500\n",
        "lr=0.4\n",
        "logistic_loss=[]\n",
        "num_samples=len(y)\n",
        "for i in range(n_iters):\n",
        "  z=????\n",
        "  q =?????\n",
        "  eps=0.00001\n",
        "  loss = -sum((y*np.log2(q+eps)+(np.ones(len(y))-y)*np.log2(np.ones(len(y))-q+eps)))\n",
        "  logistic_loss.append(loss)\n",
        "  y_pred=[int(ql > 0.5) for ql in q]\n",
        "\n",
        "\n",
        "  dw = ???\n",
        "  db = ????\n",
        "  weights = ????\n",
        "  bias = ?????\n",
        "\n",
        "plt.plot(range(1,n_iters),logistic_loss[1:])\n",
        "plt.xlabel(\"number of epochs\")\n",
        "plt.ylabel(\"loss\")\n",
        "#loss = sum(-(np.ones(len(y))*np.log2(q)+(np.ones(len(y))-y)*np.log2(np.ones(len(y))-q)))"
      ],
      "metadata": {
        "id": "My1xq84sUqUY"
      },
      "id": "My1xq84sUqUY",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now that we have a fitting model, we can use it to predict labels for our test items. The test reviews are in the one-hot matrix M_test. The labels for the test reviews are in the list labels_test.\n",
        "\n",
        "Problem 4: Complete the code below so that it calculate the vector of predicted values y_test_pred for our test items."
      ],
      "metadata": {
        "id": "-l-FYqk3f58c"
      },
      "id": "-l-FYqk3f58c"
    },
    {
      "cell_type": "code",
      "source": [
        "  z= ????\n",
        "  q = ????\n",
        "  y_test_pred=[int(ql > 0.5) for ql in q]"
      ],
      "metadata": {
        "id": "Wcl2h0B3I4lk"
      },
      "id": "Wcl2h0B3I4lk",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can calculate accuracy for the performance of our model on the test items as follows:"
      ],
      "metadata": {
        "id": "m77hqcRzgn8K"
      },
      "id": "m77hqcRzgn8K"
    },
    {
      "cell_type": "code",
      "source": [
        "y_test=[int(l == \"positive\") for l in labels_test]\n",
        "acc_test=[int(yp == y_test[s]) for s,yp in enumerate(y_test_pred)]\n",
        "print(sum(acc_test)/len(acc_test))"
      ],
      "metadata": {
        "id": "ZOz9DW4LI97R"
      },
      "id": "ZOz9DW4LI97R",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Remember though that precision is not usually a good measure and so we calculate precision and recall.\n",
        "\n",
        "Problem 5 : Calculate precision and recall values for the performance of our model on the test data. I have given code for calculating the true positive rate. You will need to calculate the rest of the values from the confusion matrix and then use these numbers to calculate our evaluation metrics."
      ],
      "metadata": {
        "id": "zs9xzOnSgw3C"
      },
      "id": "zs9xzOnSgw3C"
    },
    {
      "cell_type": "code",
      "source": [
        "labels_test_pred=[\"positive\" if s == 1 else \"negative\" for s in y_test_pred]"
      ],
      "metadata": {
        "id": "hjd74SwOLUYg"
      },
      "id": "hjd74SwOLUYg",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "true_positives=sum([int(yp == \"positive\" and labels_test[s] == \"positive\") for s,yp in enumerate(labels_test_pred)])"
      ],
      "metadata": {
        "id": "X1Nc267eNiAA"
      },
      "id": "X1Nc267eNiAA",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Problem 6: Calculate precision and recall values for the performance of the model on the training data. How do the numbers differ from those you found for the test set? Why do you think this is?"
      ],
      "metadata": {
        "id": "UnGlulqXhPd-"
      },
      "id": "UnGlulqXhPd-"
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "tQVIH4rWi_z7"
      },
      "id": "tQVIH4rWi_z7",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.9"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
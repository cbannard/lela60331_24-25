{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d3e70908"
      },
      "source": [
        "# Week 8 Seminar: From Binary Logistic Regression to Multiclass Logistic Regression and Multilayer Neural Networks\n",
        "\n"
      ],
      "id": "d3e70908"
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "We finished last week's seminar by building a logistic regression model for a sentiment classification task. The dataset used wass 10000 reviews on Yelp classified as negative (1 or 2 star) or positive (3 or 4 star).\n",
        "\n",
        "The code for reading in, preprocessing, one-hot encoding and spliting the data is as follows. Please run this one cell now as it will take a minute or two to complete"
      ],
      "metadata": {
        "id": "2GpHBpgqczHV"
      },
      "id": "2GpHBpgqczHV"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1d0fbf90",
      "metadata": {
        "id": "1d0fbf90"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "!wget https://raw.githubusercontent.com/cbannard/lela60331_24-25/refs/heads/main/data/yelp_reviews.txt\n",
        "\n",
        "# Create lists\n",
        "reviews=[]\n",
        "labels=[]\n",
        "\n",
        "with open(\"yelp_reviews.txt\") as f:\n",
        "   # iterate over the lines in the file\n",
        "   for line in f.readlines()[1:]:\n",
        "        # split the current line into a list of two element - the review and the label\n",
        "        fields = line.rstrip().split('\\t')\n",
        "        # put the current review in the reviews list\n",
        "        reviews.append(fields[0])\n",
        "        # put the current sentiment rating in the labels list\n",
        "        labels.append(fields[1])\n",
        "\n",
        "from collections import Counter\n",
        "import re\n",
        "\n",
        "#### This is where we define the character to split on\n",
        "token_def = re.compile(\"[^ ]+\")\n",
        "####\n",
        "\n",
        "# Tokenise the text, turning a list of strings into a list of lists of tokens. We use very naive space-based tokenisation.\n",
        "tokenized_sents = [token_def.findall(txt) for txt in reviews]\n",
        "# Collapse all tokens into a single list\n",
        "tokens=[]\n",
        "for s in tokenized_sents:\n",
        "      tokens.extend(s)\n",
        "# Count the tokens in the tokens list. The returns a list of tuples of each token and count\n",
        "counts=Counter(tokens)\n",
        "# Sort the tuples. The reverse argument instructs to put most frequent first rather than last (which is the default)\n",
        "so=sorted(counts.items(), key=lambda item: item[1], reverse=True)\n",
        "# Extract the list of tokens, by transposing the list of lists so that there is a list of tokens a list of counts and then just selecting the former\n",
        "so=list(zip(*so))[0]\n",
        "# Select the firs 5000 words in the list\n",
        "type_list=so[0:5000]\n",
        "\n",
        "# Create a 10000 x 5000 matrix of zeros\n",
        "M = np.zeros((len(reviews), len(type_list)))\n",
        "#iterate over the reviews\n",
        "for i, rev in enumerate(reviews):\n",
        "    # Tokenise the current review:\n",
        "    tokens = token_def.findall(rev)\n",
        "    # iterate over the words in our type list (the set of 5000 words):\n",
        "    for j,t in enumerate(type_list):\n",
        "        # if the current word j occurs in the current review i then set the matrix element at i,j to be one. Otherwise leave as zero.\n",
        "        if t in tokens:\n",
        "              M[i,j] = 1\n",
        "\n",
        "train_ints=np.random.choice(len(reviews),int(len(reviews)*0.8),replace=False)\n",
        "test_ints=list(set(range(0,len(reviews))) - set(train_ints))\n",
        "M_train = M[train_ints,]\n",
        "M_test = M[test_ints,]\n",
        "labels_train = [labels[i] for i in train_ints]\n",
        "labels_test = [labels[i] for i in test_ints]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We then built a logistic regression model. The code is below:"
      ],
      "metadata": {
        "id": "Keub330AfSJv"
      },
      "id": "Keub330AfSJv"
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "\n",
        "num_features=5000\n",
        "y=[int(l == \"positive\") for l in labels_train]\n",
        "weights = np.random.rand(num_features)\n",
        "bias=np.random.rand(1)\n",
        "n_iters = 2500\n",
        "lr=0.1\n",
        "logistic_loss=[]\n",
        "num_samples=len(y)\n",
        "for i in range(n_iters):\n",
        "  z = M_train.dot(weights)+bias\n",
        "  q = 1/(1+np.exp(-z))\n",
        "  eps=0.00001\n",
        "  loss = -sum((y*np.log2(q+eps)+(np.ones(len(y))-y)*np.log2(np.ones(len(y))-q+eps)))\n",
        "  logistic_loss.append(loss)\n",
        "\n",
        "  dw = ((q-y).dot(M_train) * (1/num_samples))\n",
        "  db = sum((q-y))/num_samples\n",
        "  weights = weights - lr*dw\n",
        "  bias = bias - lr*db\n",
        "\n",
        "plt.plot(range(1,n_iters),logistic_loss[1:])\n",
        "plt.xlabel(\"number of epochs\")\n",
        "plt.ylabel(\"loss\")\n",
        "#loss = sum(-(np.ones(len(y))*np.log2(q)+(np.ones(len(y))-y)*np.log2(np.ones(len(y))-q)))"
      ],
      "metadata": {
        "id": "My1xq84sUqUY"
      },
      "id": "My1xq84sUqUY",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now that we have a fitting model, we can use it to predict labels for our test items. The test reviews are in the one-hot matrix M_test. The labels for the test reviews are in the list labels_test.\n",
        "\n",
        "Problem 1: Complete the code below so that it calculate the vector of predicted values (0 or 1) y_test_pred for our test items."
      ],
      "metadata": {
        "id": "-l-FYqk3f58c"
      },
      "id": "-l-FYqk3f58c"
    },
    {
      "cell_type": "code",
      "source": [
        "  z = ??\n",
        "  q = ??\n",
        "  y_test_pred ??"
      ],
      "metadata": {
        "id": "Wcl2h0B3I4lk"
      },
      "id": "Wcl2h0B3I4lk",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can calculate accuracy for the performance of our model on the test items as follows:"
      ],
      "metadata": {
        "id": "m77hqcRzgn8K"
      },
      "id": "m77hqcRzgn8K"
    },
    {
      "cell_type": "code",
      "source": [
        "y_test=[int(l == \"positive\") for l in labels_test]\n",
        "acc_test=[int(yp == y_test[s]) for s,yp in enumerate(y_test_pred)]\n",
        "print(sum(acc_test)/len(acc_test))"
      ],
      "metadata": {
        "id": "ZOz9DW4LI97R"
      },
      "id": "ZOz9DW4LI97R",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Remember though that precision is not usually a good measure and so we calculate precision and recall.\n",
        "\n",
        "Problem 2 : Calculate precision and recall values for the performance of our model on the test data. I have given code for calculating the true positive rate. You will need to calculate the rest of the values from the confusion matrix and then use these numbers to calculate our evaluation metrics."
      ],
      "metadata": {
        "id": "zs9xzOnSgw3C"
      },
      "id": "zs9xzOnSgw3C"
    },
    {
      "cell_type": "code",
      "source": [
        "labels_test_pred=[\"positive\" if s == 1 else \"negative\" for s in y_test_pred]"
      ],
      "metadata": {
        "id": "hjd74SwOLUYg"
      },
      "id": "hjd74SwOLUYg",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "true_positives=sum([int(yp == \"positive\" and labels_test[s] == \"positive\") for s,yp in enumerate(labels_test_pred)])"
      ],
      "metadata": {
        "id": "X1Nc267eNiAA"
      },
      "id": "X1Nc267eNiAA",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Problem 3: Calculate precision and recall values for the performance of the model on the training data. How do the numbers differ from those you found for the test set? Why do you think this is?"
      ],
      "metadata": {
        "id": "UnGlulqXhPd-"
      },
      "id": "UnGlulqXhPd-"
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "HIJY70988eDp"
      },
      "id": "HIJY70988eDp",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Inspecting weights"
      ],
      "metadata": {
        "id": "Ms6Da17Jv6wY"
      },
      "id": "Ms6Da17Jv6wY"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Inspecting weights\n",
        "Weights in a one-layer network with one-hot encoded inputs are simple to interpret - words with high weights are those whose presence makes a positive outcome most likely and those with low weights are those whose presence makes a negative outcome most likely.\n",
        "\n",
        "Our weights are in a numpy array, with each weight corresponding the word at the same position (index) in our type_list.\n",
        "\n",
        " We can obtain a list of the indices in order to the weight at each position using the numpy function argsort. This sorts from low to high so we take the top 10 to find the indices of the ten words most strongly associated with a negative outcome. Or we can reverse it to find the indices of the ten words most strongly associated with a positive outcome.\n",
        "\n",
        " We can ten just look at the words that occur at these indices:"
      ],
      "metadata": {
        "id": "Wz84asazpYcW"
      },
      "id": "Wz84asazpYcW"
    },
    {
      "cell_type": "code",
      "source": [
        "[type_list[x] for x in np.argsort(weights)[0:20]]"
      ],
      "metadata": {
        "id": "_n7q8sz2yj5s"
      },
      "id": "_n7q8sz2yj5s",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "[type_list[x] for x in np.argsort(weights)[::-1][0:20]]"
      ],
      "metadata": {
        "id": "TjRGV1itofiD"
      },
      "id": "TjRGV1itofiD",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Problem 4a: Inspect the list of tokens with highest and lowest weights, going beyond the top and bottom 10. Do you spot any that indicate overfitting?\n",
        "\n",
        "Problem 4b: Look back up to the first code block above where we create the one-hot encoded data. Can you change the way the text is tokenised in order to avoid the problematic tokens you noted in 4a? Retrain the model. How do your changes affect performance / the token lists?\n"
      ],
      "metadata": {
        "id": "9HOxf9Qn1lP8"
      },
      "id": "9HOxf9Qn1lP8"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Exclusive OR problem\n",
        "\n",
        "Problem 5: Create the data for the AND, OR, and XOR functions. Fit a logistic regression to these problem using the code you have developed in previous problems and then inspect the output. What do you see?"
      ],
      "metadata": {
        "id": "FdW7tjtNxhRy"
      },
      "id": "FdW7tjtNxhRy"
    },
    {
      "cell_type": "code",
      "source": [
        "x=np.array([])\n",
        "y=np.array([])"
      ],
      "metadata": {
        "id": "bEBOVkO-y9x9"
      },
      "id": "bEBOVkO-y9x9",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Your first multilayer network\n",
        "\n",
        "Problem 6: Enter the weights from the example multilayer network from the lecture and demonstrate that it can solve the XOR problem\n",
        "Remember that you have two sets of weights - those from layer 0 to layer 1 and those from layer 1 to layer 2 - and that the former is a matrix not a vector.\n"
      ],
      "metadata": {
        "id": "fCH_0E_xx2Uk"
      },
      "id": "fCH_0E_xx2Uk"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Multiclass classification problems\n",
        "\n",
        "While logistic regression is great for binary classification tasks, many classification problems have more than two possible outcomes.  We can simulate such a situation as follows. I have just generalised sentiment analysis to a three class problem - negative, neutral and positive.\n",
        "\n"
      ],
      "metadata": {
        "id": "58xkbBkUzkVW"
      },
      "id": "58xkbBkUzkVW"
    },
    {
      "cell_type": "code",
      "source": [
        "## Create simulated data\n",
        "np.random.seed(10)\n",
        "w1_center = (1, 3)\n",
        "w2_center = (3, 1)\n",
        "w3_center = (1, 1)\n",
        "w4_center = (3, 3)\n",
        "\n",
        "x=np.concatenate((np.random.normal(loc=w1_center,size=(20,2)),np.random.normal(loc=w2_center,size=(20,2)),np.random.normal(loc=w3_center,size=(10,2)),np.random.normal(loc=w4_center,size=(10,2))))\n",
        "labs=np.repeat([0,1,2],[20,20,20],axis=0)\n",
        "y=np.repeat(np.diag((1,1,1)),[20,20,20],axis=0)\n",
        "x=x.T\n",
        "y=y.T"
      ],
      "metadata": {
        "id": "k-yx9h_40DIl"
      },
      "id": "k-yx9h_40DIl",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.scatter(x[0][labs==0], x[1][labs==0], marker='*', s=100)\n",
        "plt.scatter(x[0][labs==1], x[1][labs==1], marker='o', s=100)\n",
        "plt.scatter(x[0][labs==2], x[1][labs==2], marker='x', s=100)\n",
        "plt.xlabel(\"log count of negative words\")\n",
        "plt.ylabel(\"log count of positive words\")\n",
        "plt.xlim((0,5))\n",
        "plt.ylim((0,5))\n"
      ],
      "metadata": {
        "id": "I3japJAV50NE"
      },
      "id": "I3japJAV50NE",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "### Softmax\n",
        "In such circumstances we need to use multinomial logistic (aka softmax) regression.\n",
        "\n",
        "In logistic regression we take the dot product between our feature vector for each data point and our weight vector. We then add the bias to give us a single z value which we feed through the sigmoid function. We can have only one z values because there are only two outcomes and the following relationship holds:\n",
        "p(y=0|x) = 1-p(y-1)\n",
        "\n",
        "In multinomial regression we instead have a z value for each of our possible outcomes. We can use these collectively to calculate probabilties for each of our possible outcomes. For example if we had three possible outcomes, 0, 1 or 2 then we would calculate their probabilities as follows:\n",
        "\n",
        "$p(y=0|x) = \\frac{exp(z_{0})}{\\sum_{i,N} exp(z_i)}$ \\\\\n",
        "$p(y=1|x) = \\frac{exp(z_{1})}{\\sum_{i,N} exp(z_i)}$ \\\\\n",
        "$p(y=2|x) = \\frac{exp(z_{2})}{\\sum_{i,N} exp(z_i)}$ \\\\\n"
      ],
      "metadata": {
        "id": "v6QacNfX8TPj"
      },
      "id": "v6QacNfX8TPj"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Problem 7: A fitted model might return the following weights. In Python calculate the probabilites of each of the output classes for the following inputs. \\\\\n",
        "\n",
        "\n",
        "a) x[0] (positive words) = 10, x[1] (negative words) = 3 \\\\\n",
        "a) x[0] (positive words) = 3, x[1] (negative words) = 3 \\\\\n",
        "a) x[0] (positive words) = 1, x[1] (negative words) = 6 \\\\\n"
      ],
      "metadata": {
        "id": "gF0M7lm6k3O-"
      },
      "id": "gF0M7lm6k3O-"
    },
    {
      "cell_type": "code",
      "source": [
        "bias_negative=-0.82031125\n",
        "bias_positive=-0.451126\n",
        "bias_neutral = 1.27143725\n",
        "\n",
        "weights_negative = np.array([-0.69900716, 1.81182487])\n",
        "weights_positive = np.array([1.7979912 , -0.74611263])\n",
        "weights_neutral = np.array([0.80449184, -0.07135976])"
      ],
      "metadata": {
        "id": "ckqrbHUIHlON"
      },
      "id": "ckqrbHUIHlON",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Note: for convenience you can print a float with scientific notation with the  function np.format_float_positional, as in the following:"
      ],
      "metadata": {
        "id": "6WD2NE3nOYX4"
      },
      "id": "6WD2NE3nOYX4"
    },
    {
      "cell_type": "code",
      "source": [
        "x=1/783618\n",
        "x"
      ],
      "metadata": {
        "id": "bWU6LISqOd0t"
      },
      "id": "bWU6LISqOd0t",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "edCcqfqYQcmN"
      },
      "id": "edCcqfqYQcmN"
    },
    {
      "cell_type": "code",
      "source": [
        "np.format_float_positional(x)"
      ],
      "metadata": {
        "id": "-A8mqC3fOiEO"
      },
      "id": "-A8mqC3fOiEO",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Representing multinomial logistic regression problems\n",
        "\n",
        "In multinomial logistic regression we have multiple outcome classes. In place of the single 0 or 1 that we used as outcome in binary logistic regression, we represent the outcome using a vector of 0s and 1, with each position in the vector corresponding to one of the output classes. \\\\\n",
        "\n",
        "positive = [1,0,0] \\\\\n",
        "negative = [0,1,0] \\\\\n",
        "neutral = [0,0,1] \\\\\n",
        "\n",
        "This is how the y variable looks in our simulated data:"
      ],
      "metadata": {
        "id": "BG8JpQ-yPbE7"
      },
      "id": "BG8JpQ-yPbE7"
    },
    {
      "cell_type": "code",
      "source": [
        "y"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xft3loAtPYEY",
        "outputId": "2e2e5570-a9f0-41a9-fb49-dbbe32f01217"
      },
      "id": "Xft3loAtPYEY",
      "execution_count": 94,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1,\n",
              "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0,\n",
              "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
              "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1,\n",
              "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])"
            ]
          },
          "metadata": {},
          "execution_count": 94
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y.T[1:20]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hcSULKmmQeF9",
        "outputId": "b2170921-f388-4410-e07e-e40375122617"
      },
      "id": "hcSULKmmQeF9",
      "execution_count": 95,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[1, 0, 0],\n",
              "       [1, 0, 0],\n",
              "       [1, 0, 0],\n",
              "       [1, 0, 0],\n",
              "       [1, 0, 0],\n",
              "       [1, 0, 0],\n",
              "       [1, 0, 0],\n",
              "       [1, 0, 0],\n",
              "       [1, 0, 0],\n",
              "       [1, 0, 0],\n",
              "       [1, 0, 0],\n",
              "       [1, 0, 0],\n",
              "       [1, 0, 0],\n",
              "       [1, 0, 0],\n",
              "       [1, 0, 0],\n",
              "       [1, 0, 0],\n",
              "       [1, 0, 0],\n",
              "       [1, 0, 0],\n",
              "       [1, 0, 0]])"
            ]
          },
          "metadata": {},
          "execution_count": 95
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Fitting multinomial logistic regression models\n",
        "\n",
        "The relationship between this representation and the way we represent binary logistic regression is helpful in generalising the process of model fitting:\n",
        "\n",
        "Instead of using a regression equation to predict a single z value, in an n-class classifying we use n regression equations to predict n z values. These are then converted to n probabilties using the softmax function.\n",
        "\n",
        "When calculating loss and gradients in binary logistic regression we look at\n",
        "the difference between a single probability estimate and a single binary value for each datapoint for each of the m weights/input features. So that the gradient that we use to update a weight i is the mean of the following value over all datapoints:\n",
        "\n",
        "$ g_i =  (q - y) * x_{i}  \\\\ =     (p(y = 1|x) - y) * x_{i}$\n",
        "\n",
        "In multinomial logistic regression, we compare each of our n probabilities to each of n binary values when updating each of our n x k weights for each of our k input features.  So for m classes 0, 1 and 2 we would calculate the average of the following over all datapoints:\n",
        "\n",
        "$ g_{i}^{0} = (q^{0} - y^{0}) * x_{i}    =     (p(y^{0} = 1|x) - y^{0}) * x_{i}$\n",
        "\n",
        "$ g_{i}^{1} = (q^{1} - y^{1}) * x_{i}    =     (p(y^{1} = 1|x) - y^{0}) * x_{i}$  \n",
        "\n",
        "$ g_{i}^{2} = (q^{2} - y^{2}) * x_{i}    =     (p(y^{2} = 1|x) - y^{0}) * x_{i}$  \n"
      ],
      "metadata": {
        "id": "SBaLVvbiQkb4"
      },
      "id": "SBaLVvbiQkb4"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Problem 8: Complete the code below so that it fits a softmax regression to our multiclass data"
      ],
      "metadata": {
        "id": "ZW-uMSS9OSyV"
      },
      "id": "ZW-uMSS9OSyV"
    },
    {
      "cell_type": "code",
      "source": [
        "np.random.seed(10)\n",
        "n_iters = 2500\n",
        "num_features=2\n",
        "num_classes=3\n",
        "num_samples = len(y[0])\n",
        "weights = np.random.rand(num_classes,num_features)\n",
        "bias=np.zeros(num_classes)\n",
        "lr=0.1\n",
        "logistic_loss=[]\n",
        "z=np.zeros((num_samples,num_classes))\n",
        "q=np.zeros((num_samples,num_classes))\n",
        "\n",
        "for i in range(n_iters):\n",
        "    z[:,0]=???\n",
        "    z[:,1]=???\n",
        "    z[:,2]=???\n",
        "\n",
        "    q[:,0] = ???\n",
        "    q[:,1] = ???\n",
        "    q[:,2] = ???\n",
        "\n",
        "    loss = sum(-(y[0]*np.log2(q[:,0])+(1-y[0])*np.log2(1-q[:,0])))/num_samples\n",
        "    loss += sum(-(y[1]*np.log2(q[:,1])+(1-y[1])*np.log2(1-q[:,1])))/num_samples\n",
        "    loss += sum(-(y[2]*np.log2(q[:,2])+(1-y[2])*np.log2(1-q[:,2])))/num_samples\n",
        "    logistic_loss.append(loss)\n",
        "\n",
        "    dw01 = ???\n",
        "    dw02 = ???\n",
        "\n",
        "    dw11 = ???\n",
        "    dw12 = ???\n",
        "\n",
        "    dw21 = ???\n",
        "    dw22 = ???\n",
        "\n",
        "    db0 = ???\n",
        "    db1 = ???\n",
        "    db2 = ???\n",
        "\n",
        "    weights[0,0] = weights[0,0] - dw01*lr\n",
        "    weights[0,1] = weights[0,1] - dw02*lr\n",
        "\n",
        "    weights[1,0] = weights[1,0] - dw11*lr\n",
        "    weights[1,1] = weights[1,1] - dw12*lr\n",
        "\n",
        "    weights[2,0] = weights[2,0] - dw21*lr\n",
        "    weights[2,1] = weights[2,1] - dw22*lr\n",
        "\n",
        "    bias[0] = bias[0] - db0*lr\n",
        "    bias[1] = bias[1] - db1*lr\n",
        "    bias[2] = bias[2] - db2*lr\n",
        "\n",
        "plt.plot(range(1,n_iters),logistic_loss[1:])\n",
        "plt.xlabel(\"number of epochs\")\n",
        "plt.ylabel(\"loss\")"
      ],
      "metadata": {
        "id": "mtUjZzhE60CO"
      },
      "id": "mtUjZzhE60CO",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Problem 9 (difficult addition to try in your own time if you feel able). Rewrite your softmax regression so that it calculates z, q, gradients and weights in single lines using operations over whole matrices rather than subsetting for particular rows, columns or elements. See assigned readings on linear algebra to remind you of numpy operations if needed."
      ],
      "metadata": {
        "id": "aa6Hk7w1sv5B"
      },
      "id": "aa6Hk7w1sv5B"
    },
    {
      "cell_type": "code",
      "source": [
        "np.random.seed(10)\n",
        "n_iters = 2500\n",
        "num_features=2\n",
        "num_classes=3\n",
        "num_samples = len(y[0])\n",
        "weights = np.random.rand(num_classes,num_features)\n",
        "bias=np.zeros(num_classes)\n",
        "lr=0.1\n",
        "logistic_loss=[]\n",
        "z=np.zeros((num_samples,num_classes))\n",
        "q=np.zeros((num_samples,num_classes))\n",
        "\n",
        "for i in range(n_iters):\n",
        "    z=???\n",
        "\n",
        "    q = ???\n",
        "    q = ???\n",
        "    q = ???\n",
        "\n",
        "    loss = sum(-(y[0]*np.log2(q[:,0])+(1-y[0])*np.log2(1-q[:,0])))/num_samples\n",
        "    loss += sum(-(y[1]*np.log2(q[:,1])+(1-y[1])*np.log2(1-q[:,1])))/num_samples\n",
        "    loss += sum(-(y[2]*np.log2(q[:,2])+(1-y[2])*np.log2(1-q[:,2])))/num_samples\n",
        "    logistic_loss.append(loss)\n",
        "\n",
        "    dw = ??\n",
        "    db = ??\n",
        "\n",
        "\n",
        "    weights = ??\n",
        "    bias = ??\n",
        "\n",
        "plt.plot(range(1,n_iters),logistic_loss[1:])\n",
        "plt.xlabel(\"number of epochs\")\n",
        "plt.ylabel(\"loss\")"
      ],
      "metadata": {
        "id": "NOsmfXmkthPB"
      },
      "id": "NOsmfXmkthPB",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.9"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}